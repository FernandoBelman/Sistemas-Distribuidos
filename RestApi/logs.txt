
==> Audit <==
|---------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:47 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:49 CST | 21 Nov 24 13:49 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:49 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| start   | --memory=1500 --cpus=2         | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:56 CST |                     |
|         | --insecure-registry            |          |                       |         |                     |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:56 CST | 21 Nov 24 13:56 CST |
| start   | --memory=1500 --cpus=2         | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:56 CST |                     |
|         | --insecure-registry            |          |                       |         |                     |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:57 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:58 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
|         | --memory=3072                  |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:59 CST | 21 Nov 24 13:59 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 13:59 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
|         | --memory=3072                  |          |                       |         |                     |                     |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 14:05 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 14:05 CST | 21 Nov 24 14:05 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 14:05 CST | 21 Nov 24 14:09 CST |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| addons  | enable registry                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 14:21 CST | 21 Nov 24 14:21 CST |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:12 CST |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:22 CST |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:37 CST |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:39 CST | 21 Nov 24 16:40 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:40 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:40 CST | 21 Nov 24 16:40 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:41 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
|         | --memory=3072                  |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:42 CST | 21 Nov 24 16:42 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:42 CST | 21 Nov 24 16:44 CST |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| addons  | enable registry                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 16:46 CST | 21 Nov 24 16:47 CST |
| stop    |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 17:36 CST | 21 Nov 24 17:36 CST |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 17:46 CST | 21 Nov 24 17:46 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 18:28 CST | 21 Nov 24 18:30 CST |
|         | 10.0.0.0/24                    |          |                       |         |                     |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 18:40 CST | 21 Nov 24 18:41 CST |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 18:50 CST | 21 Nov 24 18:51 CST |
| stop    |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 19:00 CST | 21 Nov 24 19:00 CST |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:05 CST | 21 Nov 24 22:05 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:06 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:06 CST | 21 Nov 24 22:06 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:06 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
|         | --memory=3072                  |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:10 CST | 21 Nov 24 22:10 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:12 CST |                     |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
| delete  |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:16 CST | 21 Nov 24 22:16 CST |
| start   | --insecure-registry            | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:16 CST | 21 Nov 24 22:20 CST |
|         | 10.0.0.0/24 --driver=hyperv    |          |                       |         |                     |                     |
|         | --memory=3072                  |          |                       |         |                     |                     |
| addons  | enable registry                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:21 CST | 21 Nov 24 22:23 CST |
| service | groups-api-svc -n backend-api  | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:40 CST |                     |
| service | groups-api-svc -n backend-api  | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 22:42 CST |                     |
| service | groups-api-svc -n backend-api  | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 21 Nov 24 23:38 CST |                     |
| service | groups-api-svc -n backend-api  | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:05 CST | 22 Nov 24 00:06 CST |
| addons  | enable registry                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:07 CST | 22 Nov 24 00:07 CST |
| addons  | enable ingress                 | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:08 CST | 22 Nov 24 00:09 CST |
| addons  | enable ingress-dns             | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:09 CST | 22 Nov 24 00:09 CST |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:10 CST | 22 Nov 24 00:10 CST |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:10 CST | 22 Nov 24 00:10 CST |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:11 CST |                     |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:11 CST |                     |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:12 CST | 22 Nov 24 00:12 CST |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:14 CST | 22 Nov 24 00:14 CST |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:15 CST | 22 Nov 24 00:15 CST |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:43 CST |                     |
| ip      |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 22 Nov 24 00:43 CST | 22 Nov 24 00:43 CST |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 26 Nov 24 17:23 CST |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 26 Nov 24 17:24 CST |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 26 Nov 24 17:41 CST |                     |
| start   |                                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 26 Nov 24 17:56 CST |                     |
| start   | --driver=hyperv                | minikube | LAPTOP-AA72QI8I\lbelm | v1.34.0 | 26 Nov 24 17:58 CST |                     |
|---------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/11/26 17:58:57
Running on machine: LAPTOP-AA72QI8I
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1126 17:58:57.205466    6956 out.go:345] Setting OutFile to fd 84 ...
I1126 17:58:57.215483    6956 out.go:358] Setting ErrFile to fd 88...
I1126 17:58:57.231475    6956 out.go:352] Setting JSON to false
I1126 17:58:57.235523    6956 start.go:129] hostinfo: {"hostname":"LAPTOP-AA72QI8I","uptime":332952,"bootTime":1732332585,"procs":225,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5131 Build 19045.5131","kernelVersion":"10.0.19045.5131 Build 19045.5131","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e5f985b1-b645-4efd-9e5b-ec7bb58aaaa1"}
W1126 17:58:57.235523    6956 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1126 17:58:57.238640    6956 out.go:177] üòÑ  minikube v1.34.0 en Microsoft Windows 10 Pro 10.0.19045.5131 Build 19045.5131
I1126 17:58:57.243274    6956 notify.go:220] Checking for updates...
I1126 17:58:57.244822    6956 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1126 17:58:57.245688    6956 driver.go:394] Setting default libvirt URI to qemu:///system
I1126 17:59:00.352739    6956 out.go:177] ‚ú®  Using the hyperv driver based on existing profile
I1126 17:59:00.356098    6956 start.go:297] selected driver: hyperv
I1126 17:59:00.356098    6956 start.go:901] validating driver "hyperv" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3072 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.0.0.0/24] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.30.130.31 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true ingress-dns:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\lbelm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1126 17:59:00.356098    6956 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1126 17:59:00.385520    6956 cni.go:84] Creating CNI manager for ""
I1126 17:59:00.385520    6956 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1126 17:59:00.385520    6956 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3072 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.0.0.0/24] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.30.130.31 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true ingress-dns:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\lbelm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1126 17:59:00.386043    6956 iso.go:125] acquiring lock: {Name:mk11a3eebe6b611f6ba1fb86bdad31f0b8685048 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1126 17:59:00.389800    6956 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1126 17:59:00.395625    6956 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1126 17:59:00.395625    6956 preload.go:146] Found local preload: C:\Users\lbelm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1126 17:59:00.395625    6956 cache.go:56] Caching tarball of preloaded images
I1126 17:59:00.396172    6956 preload.go:172] Found C:\Users\lbelm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1126 17:59:00.396172    6956 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1126 17:59:00.396716    6956 profile.go:143] Saving config to C:\Users\lbelm\.minikube\profiles\minikube\config.json ...
I1126 17:59:00.398294    6956 start.go:360] acquireMachinesLock for minikube: {Name:mk21520c39d113fa5c80e5f035f4292580b3c623 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1126 17:59:00.398294    6956 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1126 17:59:00.398294    6956 start.go:96] Skipping create...Using existing machine configuration
I1126 17:59:00.398294    6956 fix.go:54] fixHost starting: 
I1126 17:59:00.398829    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:01.934110    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:01.934110    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:01.934110    6956 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1126 17:59:01.934110    6956 fix.go:138] unexpected machine state, will restart: <nil>
I1126 17:59:01.938373    6956 out.go:177] üèÉ  Updating the running hyperv "minikube" VM ...
I1126 17:59:01.943202    6956 machine.go:93] provisionDockerMachine start ...
I1126 17:59:01.943202    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:03.187489    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:03.187489    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:03.187489    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:04.705082    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:04.705082    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:04.709071    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:04.709599    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:04.709599    6956 main.go:141] libmachine: About to run SSH command:
hostname
I1126 17:59:04.814599    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1126 17:59:04.814599    6956 buildroot.go:166] provisioning hostname "minikube"
I1126 17:59:04.814599    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:06.017214    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:06.017214    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:06.017214    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:07.525756    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:07.525756    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:07.530702    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:07.530702    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:07.530702    6956 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1126 17:59:07.638266    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1126 17:59:07.638266    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:08.851317    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:08.851317    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:08.851436    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:10.339695    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:10.339695    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:10.345376    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:10.345376    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:10.345376    6956 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1126 17:59:10.453216    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1126 17:59:10.453216    6956 buildroot.go:172] set auth options {CertDir:C:\Users\lbelm\.minikube CaCertPath:C:\Users\lbelm\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\lbelm\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\lbelm\.minikube\machines\server.pem ServerKeyPath:C:\Users\lbelm\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\lbelm\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\lbelm\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\lbelm\.minikube}
I1126 17:59:10.453216    6956 buildroot.go:174] setting up certificates
I1126 17:59:10.453216    6956 provision.go:84] configureAuth start
I1126 17:59:10.453216    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:11.691201    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:11.691201    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:11.691201    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:13.166587    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:13.166595    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:13.166688    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:14.375339    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:14.375339    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:14.375339    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:15.870621    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:15.870858    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:15.870858    6956 provision.go:143] copyHostCerts
I1126 17:59:15.871188    6956 exec_runner.go:144] found C:\Users\lbelm\.minikube/ca.pem, removing ...
I1126 17:59:15.871188    6956 exec_runner.go:203] rm: C:\Users\lbelm\.minikube\ca.pem
I1126 17:59:15.871410    6956 exec_runner.go:151] cp: C:\Users\lbelm\.minikube\certs\ca.pem --> C:\Users\lbelm\.minikube/ca.pem (1074 bytes)
I1126 17:59:15.871544    6956 exec_runner.go:144] found C:\Users\lbelm\.minikube/cert.pem, removing ...
I1126 17:59:15.871544    6956 exec_runner.go:203] rm: C:\Users\lbelm\.minikube\cert.pem
I1126 17:59:15.872117    6956 exec_runner.go:151] cp: C:\Users\lbelm\.minikube\certs\cert.pem --> C:\Users\lbelm\.minikube/cert.pem (1119 bytes)
I1126 17:59:15.872677    6956 exec_runner.go:144] found C:\Users\lbelm\.minikube/key.pem, removing ...
I1126 17:59:15.872677    6956 exec_runner.go:203] rm: C:\Users\lbelm\.minikube\key.pem
I1126 17:59:15.872677    6956 exec_runner.go:151] cp: C:\Users\lbelm\.minikube\certs\key.pem --> C:\Users\lbelm\.minikube/key.pem (1679 bytes)
I1126 17:59:15.873241    6956 provision.go:117] generating server cert: C:\Users\lbelm\.minikube\machines\server.pem ca-key=C:\Users\lbelm\.minikube\certs\ca.pem private-key=C:\Users\lbelm\.minikube\certs\ca-key.pem org=lbelm.minikube san=[127.0.0.1 192.168.96.65 localhost minikube]
I1126 17:59:16.045466    6956 provision.go:177] copyRemoteCerts
I1126 17:59:16.061976    6956 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1126 17:59:16.062492    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:17.270453    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:17.270453    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:17.270453    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:18.756271    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:18.756271    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:18.756271    6956 sshutil.go:53] new ssh client: &{IP:192.168.96.65 Port:22 SSHKeyPath:C:\Users\lbelm\.minikube\machines\minikube\id_rsa Username:docker}
I1126 17:59:18.842874    6956 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (2.7808977s)
I1126 17:59:18.842874    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1126 17:59:18.864090    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1126 17:59:18.883577    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1126 17:59:18.904380    6956 provision.go:87] duration metric: took 8.4511647s to configureAuth
I1126 17:59:18.904380    6956 buildroot.go:189] setting minikube options for container-runtime
I1126 17:59:18.905559    6956 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1126 17:59:18.905559    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:20.129163    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:20.129163    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:20.129163    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:21.609756    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:21.609756    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:21.616426    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:21.616963    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:21.616963    6956 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1126 17:59:21.717486    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I1126 17:59:21.717486    6956 buildroot.go:70] root file system type: tmpfs
I1126 17:59:21.718037    6956 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1126 17:59:21.718037    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:22.944509    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:22.944509    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:22.944509    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:24.432211    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:24.432211    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:24.436950    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:24.437574    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:24.437626    6956 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 --insecure-registry 10.0.0.0/24 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1126 17:59:24.561593    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 --insecure-registry 10.0.0.0/24 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1126 17:59:24.561593    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:25.769595    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:25.769595    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:25.769595    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:27.241958    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:27.241958    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:27.246140    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:27.246172    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:27.246172    6956 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1126 17:59:27.357563    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1126 17:59:27.357563    6956 machine.go:96] duration metric: took 25.414361s to provisionDockerMachine
I1126 17:59:27.357563    6956 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I1126 17:59:27.357563    6956 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1126 17:59:27.372741    6956 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1126 17:59:27.372741    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:28.594332    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:28.594332    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:28.594332    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:30.084372    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:30.084372    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:30.084372    6956 sshutil.go:53] new ssh client: &{IP:192.168.96.65 Port:22 SSHKeyPath:C:\Users\lbelm\.minikube\machines\minikube\id_rsa Username:docker}
I1126 17:59:30.163317    6956 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (2.7905755s)
I1126 17:59:30.169360    6956 ssh_runner.go:195] Run: cat /etc/os-release
I1126 17:59:30.172992    6956 info.go:137] Remote host: Buildroot 2023.02.9
I1126 17:59:30.172992    6956 filesync.go:126] Scanning C:\Users\lbelm\.minikube\addons for local assets ...
I1126 17:59:30.173542    6956 filesync.go:126] Scanning C:\Users\lbelm\.minikube\files for local assets ...
I1126 17:59:30.173542    6956 start.go:296] duration metric: took 2.8159787s for postStartSetup
I1126 17:59:30.173542    6956 fix.go:56] duration metric: took 29.775248s for fixHost
I1126 17:59:30.173542    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:31.408889    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:31.408889    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:31.408889    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:32.878433    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:32.878433    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:32.883598    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:32.884173    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:32.884173    6956 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I1126 17:59:32.993117    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: 1732665573.379598736

I1126 17:59:32.993117    6956 fix.go:216] guest clock: 1732665573.379598736
I1126 17:59:32.993117    6956 fix.go:229] Guest: 2024-11-26 17:59:33.379598736 -0600 CST Remote: 2024-11-26 17:59:30.1735421 -0600 CST m=+33.035219801 (delta=3.206056636s)
I1126 17:59:32.993117    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:34.205295    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:34.205295    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:34.205295    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:35.671113    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:35.671165    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:35.676417    6956 main.go:141] libmachine: Using SSH client type: native
I1126 17:59:35.676992    6956 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaec9c0] 0xaef5a0 <nil>  [] 0s} 192.168.96.65 22 <nil> <nil>}
I1126 17:59:35.676992    6956 main.go:141] libmachine: About to run SSH command:
sudo date -s @1732665572
I1126 17:59:35.779178    6956 main.go:141] libmachine: SSH cmd err, output: <nil>: Tue Nov 26 23:59:32 UTC 2024

I1126 17:59:35.779178    6956 fix.go:236] clock set: Tue Nov 26 23:59:32 UTC 2024
 (err=<nil>)
I1126 17:59:35.779178    6956 start.go:83] releasing machines lock for "minikube", held for 35.3808843s
I1126 17:59:35.779178    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:37.000324    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:37.000324    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:37.000324    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:38.477958    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:38.477958    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:38.480983    6956 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1126 17:59:38.480983    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:38.483213    6956 ssh_runner.go:195] Run: cat /version.json
I1126 17:59:38.483213    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I1126 17:59:39.812808    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:39.812808    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:39.812808    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:39.826677    6956 main.go:141] libmachine: [stdout =====>] : Running

I1126 17:59:39.826677    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:39.826677    6956 main.go:141] libmachine: [executing ==>] : C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I1126 17:59:41.375860    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:41.375860    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:41.376085    6956 sshutil.go:53] new ssh client: &{IP:192.168.96.65 Port:22 SSHKeyPath:C:\Users\lbelm\.minikube\machines\minikube\id_rsa Username:docker}
I1126 17:59:41.385410    6956 main.go:141] libmachine: [stdout =====>] : 192.168.96.65

I1126 17:59:41.385410    6956 main.go:141] libmachine: [stderr =====>] : 
I1126 17:59:41.385585    6956 sshutil.go:53] new ssh client: &{IP:192.168.96.65 Port:22 SSHKeyPath:C:\Users\lbelm\.minikube\machines\minikube\id_rsa Username:docker}
I1126 17:59:41.457594    6956 ssh_runner.go:235] Completed: cat /version.json: (2.9743804s)
I1126 17:59:41.457594    6956 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (2.9766104s)
W1126 17:59:41.457594    6956 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1126 17:59:41.475295    6956 ssh_runner.go:195] Run: systemctl --version
I1126 17:59:41.485122    6956 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W1126 17:59:41.489296    6956 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I1126 17:59:41.504457    6956 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1126 17:59:41.512034    6956 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1126 17:59:41.512034    6956 start.go:495] detecting cgroup driver to use...
I1126 17:59:41.512034    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1126 17:59:41.531367    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1126 17:59:41.545981    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1126 17:59:41.554743    6956 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1126 17:59:41.559019    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1126 17:59:41.574135    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1126 17:59:41.587309    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1126 17:59:41.600933    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1126 17:59:41.613446    6956 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1126 17:59:41.626046    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
W1126 17:59:41.629083    6956 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W1126 17:59:41.629083    6956 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1126 17:59:41.639814    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1126 17:59:41.653939    6956 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1126 17:59:41.662530    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc/containerd/certs.d/10.0.0.0/24 && printf %s "c2VydmVyID0gImh0dHA6Ly8xMC4wLjAuMC8yNCIKCltob3N0LiJodHRwOi8vMTAuMC4wLjAvMjQiXQogIHNraXBfdmVyaWZ5ID0gdHJ1ZQo=" | base64 -d | sudo tee /etc/containerd/certs.d/10.0.0.0/24/hosts.toml"
I1126 17:59:41.691256    6956 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1126 17:59:41.713995    6956 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1126 17:59:41.736508    6956 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1126 17:59:41.863157    6956 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1126 17:59:41.880395    6956 start.go:495] detecting cgroup driver to use...
I1126 17:59:41.894651    6956 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1126 17:59:41.920424    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1126 17:59:41.945661    6956 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1126 17:59:41.975219    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1126 17:59:42.000707    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1126 17:59:42.011029    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1126 17:59:42.029964    6956 ssh_runner.go:195] Run: which cri-dockerd
I1126 17:59:42.047514    6956 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1126 17:59:42.055651    6956 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1126 17:59:42.082135    6956 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1126 17:59:42.213996    6956 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1126 17:59:42.342238    6956 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1126 17:59:42.342238    6956 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1126 17:59:42.370036    6956 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1126 17:59:42.495269    6956 ssh_runner.go:195] Run: sudo systemctl restart docker
I1126 17:59:55.007253    6956 ssh_runner.go:235] Completed: sudo systemctl restart docker: (12.5119841s)
I1126 17:59:55.021735    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1126 17:59:55.048870    6956 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1126 17:59:55.078340    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1126 17:59:55.106832    6956 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1126 17:59:55.222509    6956 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1126 17:59:55.345585    6956 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1126 17:59:55.465128    6956 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1126 17:59:55.497145    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1126 17:59:55.523771    6956 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1126 17:59:55.649077    6956 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1126 17:59:55.714393    6956 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1126 17:59:55.718667    6956 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1126 17:59:55.722664    6956 start.go:563] Will wait 60s for crictl version
I1126 17:59:55.727770    6956 ssh_runner.go:195] Run: which crictl
I1126 17:59:55.745948    6956 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1126 17:59:55.772450    6956 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1126 17:59:55.778343    6956 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1126 17:59:55.800682    6956 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1126 17:59:55.819570    6956 out.go:235] üê≥  Preparando Kubernetes v1.31.0 en Docker 27.2.0...
I1126 17:59:55.820109    6956 ip.go:176] getIPForInterface: searching for "vEthernet (Default Switch)"
I1126 17:59:55.823264    6956 ip.go:185] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I1126 17:59:55.823264    6956 ip.go:211] Found interface: {Index:36 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:1a:98:da Flags:up|broadcast|multicast|running}
I1126 17:59:55.825877    6956 ip.go:214] interface addr: fe80::a014:b49b:80f:bbe6/64
I1126 17:59:55.825877    6956 ip.go:214] interface addr: 192.168.96.1/20
I1126 17:59:55.830690    6956 ssh_runner.go:195] Run: grep 192.168.96.1	host.minikube.internal$ /etc/hosts
I1126 17:59:55.834539    6956 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3072 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.0.0.0/24] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.30.130.31 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true ingress-dns:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\lbelm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1126 17:59:55.834539    6956 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1126 17:59:55.840599    6956 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1126 17:59:55.855180    6956 docker.go:685] Got preloaded images: -- stdout --
localhost:5000/groups-api:1
bitnami/postgresql:17.1.0-debian-12-r0
localhost:5000/users-api:1
bitnami/mongodb:8.0.3-debian-12-r0
mysql:latest
oryd/hydra-maester:v0.0.35-amd64
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/kube-registry-proxy:<none>
oryd/hydra:v2.2.0
registry:<none>
registry.k8s.io/coredns/coredns:v1.11.1
kicbase/echo-server:1.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1126 17:59:55.855180    6956 docker.go:615] Images already preloaded, skipping extraction
I1126 17:59:55.862714    6956 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1126 17:59:55.877835    6956 docker.go:685] Got preloaded images: -- stdout --
localhost:5000/groups-api:1
bitnami/postgresql:17.1.0-debian-12-r0
localhost:5000/users-api:1
bitnami/mongodb:8.0.3-debian-12-r0
mysql:latest
oryd/hydra-maester:v0.0.35-amd64
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/kube-registry-proxy:<none>
oryd/hydra:v2.2.0
registry:<none>
registry.k8s.io/coredns/coredns:v1.11.1
kicbase/echo-server:1.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1126 17:59:55.877835    6956 cache_images.go:84] Images are preloaded, skipping loading
I1126 17:59:55.877835    6956 kubeadm.go:934] updating node { 172.30.130.31 8443 v1.31.0 docker true true} ...
I1126 17:59:55.877835    6956 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.30.130.31

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1126 17:59:55.883608    6956 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1126 17:59:55.916159    6956 cni.go:84] Creating CNI manager for ""
I1126 17:59:55.916159    6956 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1126 17:59:55.916159    6956 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1126 17:59:55.916159    6956 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.30.130.31 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.30.130.31"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.30.130.31 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1126 17:59:55.918370    6956 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.30.130.31
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.30.130.31
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.30.130.31"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1126 17:59:55.932670    6956 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1126 17:59:55.939854    6956 binaries.go:44] Found k8s binaries, skipping transfer
I1126 17:59:55.955047    6956 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1126 17:59:55.962481    6956 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (308 bytes)
I1126 17:59:55.976025    6956 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1126 17:59:55.989222    6956 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2153 bytes)
I1126 17:59:56.005617    6956 ssh_runner.go:195] Run: grep 172.30.130.31	control-plane.minikube.internal$ /etc/hosts
I1126 17:59:56.023224    6956 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1126 17:59:56.141723    6956 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1126 17:59:56.153164    6956 certs.go:68] Setting up C:\Users\lbelm\.minikube\profiles\minikube for IP: 172.30.130.31
I1126 17:59:56.153164    6956 certs.go:194] generating shared ca certs ...
I1126 17:59:56.153275    6956 certs.go:226] acquiring lock for ca certs: {Name:mka6aa39b16cc1f0f577464981388e2cf72ec8e3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1126 17:59:56.153830    6956 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\lbelm\.minikube\ca.key
I1126 17:59:56.153914    6956 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\lbelm\.minikube\proxy-client-ca.key
I1126 17:59:56.153914    6956 certs.go:256] generating profile certs ...
I1126 17:59:56.154493    6956 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\lbelm\.minikube\profiles\minikube\client.key
I1126 17:59:56.154493    6956 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\lbelm\.minikube\profiles\minikube\apiserver.key.55f26d7c
I1126 17:59:56.154493    6956 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\lbelm\.minikube\profiles\minikube\proxy-client.key
I1126 17:59:56.155067    6956 certs.go:484] found cert: C:\Users\lbelm\.minikube\certs\ca-key.pem (1679 bytes)
I1126 17:59:56.155640    6956 certs.go:484] found cert: C:\Users\lbelm\.minikube\certs\ca.pem (1074 bytes)
I1126 17:59:56.155640    6956 certs.go:484] found cert: C:\Users\lbelm\.minikube\certs\cert.pem (1119 bytes)
I1126 17:59:56.155640    6956 certs.go:484] found cert: C:\Users\lbelm\.minikube\certs\key.pem (1679 bytes)
I1126 17:59:56.157425    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1126 17:59:56.184126    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1126 17:59:56.203413    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1126 17:59:56.225335    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1126 17:59:56.245447    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1126 17:59:56.264727    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1126 17:59:56.285202    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1126 17:59:56.304138    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1126 17:59:56.324879    6956 ssh_runner.go:362] scp C:\Users\lbelm\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1126 17:59:56.344810    6956 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1126 17:59:56.373747    6956 ssh_runner.go:195] Run: openssl version
I1126 17:59:56.392390    6956 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1126 17:59:56.407104    6956 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1126 17:59:56.411153    6956 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 23 00:04 /usr/share/ca-certificates/minikubeCA.pem
I1126 17:59:56.426836    6956 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1126 17:59:56.446849    6956 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1126 17:59:56.458751    6956 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1126 17:59:56.477716    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1126 17:59:56.497269    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1126 17:59:56.518010    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1126 17:59:56.536671    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1126 17:59:56.556206    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1126 17:59:56.577263    6956 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1126 17:59:56.581513    6956 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.34.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3072 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[10.0.0.0/24] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.30.130.31 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true ingress-dns:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\lbelm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1126 17:59:56.587361    6956 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1126 17:59:56.615972    6956 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1126 17:59:56.623562    6956 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1126 17:59:56.623562    6956 kubeadm.go:593] restartPrimaryControlPlane start ...
I1126 17:59:56.638074    6956 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1126 17:59:56.646872    6956 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1126 17:59:56.647478    6956 kubeconfig.go:125] found "minikube" server: "https://172.30.130.31:8443"
I1126 17:59:56.666631    6956 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1126 17:59:56.674070    6956 kubeadm.go:630] The running cluster does not require reconfiguration: 172.30.130.31
I1126 17:59:56.674119    6956 kubeadm.go:1160] stopping kube-system containers ...
I1126 17:59:56.680212    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1126 17:59:56.693922    6956 docker.go:483] Stopping containers: [01823a606da1 3c0a5e5913a7 b49ef773eab6 e40d7d8c7596 5aa9e53c2bae ab104b9920bf 61c8c6a98b18 d1957df5bb1f 364c3e4c55a7 6696a0e241cd 6945ffe6ac0e d3eb01de9b9f 5a1797180782 32cb016b1105 04cae36552c2 b779c17402e8 dedb6d07fe7a eaf6085add98]
I1126 17:59:56.701461    6956 ssh_runner.go:195] Run: docker stop 01823a606da1 3c0a5e5913a7 b49ef773eab6 e40d7d8c7596 5aa9e53c2bae ab104b9920bf 61c8c6a98b18 d1957df5bb1f 364c3e4c55a7 6696a0e241cd 6945ffe6ac0e d3eb01de9b9f 5a1797180782 32cb016b1105 04cae36552c2 b779c17402e8 dedb6d07fe7a eaf6085add98
I1126 17:59:56.733309    6956 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1126 17:59:56.787718    6956 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1126 17:59:56.796977    6956 kubeadm.go:157] found existing configuration files:
-rw------- 1 root root 5647 Nov 26 23:50 /etc/kubernetes/admin.conf
-rw------- 1 root root 5657 Nov 26 23:57 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5655 Nov 26 23:50 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5601 Nov 26 23:57 /etc/kubernetes/scheduler.conf

I1126 17:59:56.810804    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1126 17:59:56.834082    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1126 17:59:56.857372    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1126 17:59:56.865602    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1126 17:59:56.880097    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1126 17:59:56.902175    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1126 17:59:56.909984    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1126 17:59:56.925679    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1126 17:59:56.949159    6956 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1126 17:59:56.960769    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1126 17:59:56.998880    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1126 17:59:57.864789    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1126 17:59:58.040817    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1126 17:59:58.095953    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1126 17:59:58.172702    6956 api_server.go:52] waiting for apiserver process to appear ...
I1126 17:59:58.186737    6956 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1126 17:59:58.702075    6956 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1126 17:59:59.201114    6956 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1126 17:59:59.701958    6956 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1126 17:59:59.714361    6956 api_server.go:72] duration metric: took 1.5416388s to wait for apiserver process to appear ...
I1126 17:59:59.714361    6956 api_server.go:88] waiting for apiserver healthz status ...
I1126 17:59:59.714361    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:04.724219    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:04.724219    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:09.728965    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:09.728965    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:14.745027    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:14.745027    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:19.750273    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:19.750273    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:24.760704    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:24.760704    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:29.775051    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:29.775051    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:34.775480    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:34.775480    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:39.785793    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:39.785793    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:44.790575    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:44.790575    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:49.805356    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:49.805356    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:54.810573    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:54.810573    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:00:59.826585    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:00:59.835176    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:00:59.849254    6956 logs.go:276] 1 containers: [cff135361a59]
I1126 18:00:59.855038    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:00:59.866270    6956 logs.go:276] 1 containers: [4e6391f0f1e0]
I1126 18:00:59.872501    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:00:59.884548    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:00:59.891074    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:00:59.904555    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:00:59.910365    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:00:59.922114    6956 logs.go:276] 0 containers: []
W1126 18:00:59.922114    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:00:59.927976    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:00:59.941840    6956 logs.go:276] 2 containers: [1508f888dc1a b49ef773eab6]
I1126 18:00:59.947755    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:00:59.959410    6956 logs.go:276] 0 containers: []
W1126 18:00:59.959410    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:00:59.965232    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:00:59.976755    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:00:59.983230    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:00:59.994652    6956 logs.go:276] 0 containers: []
W1126 18:00:59.994652    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:00:59.994652    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:00:59.994652    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:00.055550    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:00.055550    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:00.069856    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:00.069856    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:01:00.123839    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:00.500757   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.502539   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.504229   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.506169   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.508050   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:00.500757   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.502539   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.504229   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.506169   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:00.508050   21733 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:00.123839    6956 logs.go:123] Gathering logs for kube-apiserver [cff135361a59] ...
I1126 18:01:00.123839    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cff135361a59"
I1126 18:01:00.142906    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:00.143431    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:00.165568    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:00.166089    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:00.190600    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:00.190600    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:00.234153    6956 logs.go:123] Gathering logs for etcd [4e6391f0f1e0] ...
I1126 18:01:00.234153    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4e6391f0f1e0"
I1126 18:01:00.249879    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:00.249879    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:00.286816    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:00.286816    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:00.301342    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:00.301342    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:00.317307    6956 logs.go:123] Gathering logs for kube-controller-manager [b49ef773eab6] ...
I1126 18:01:00.317307    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b49ef773eab6"
I1126 18:01:00.330916    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:00.330916    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:02.886300    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:01:07.901990    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:01:07.909145    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:01:07.924909    6956 logs.go:276] 1 containers: [cff135361a59]
I1126 18:01:07.930791    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:01:07.946628    6956 logs.go:276] 1 containers: [4e6391f0f1e0]
I1126 18:01:07.953480    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:01:07.966774    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:01:07.972981    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:01:07.988045    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:01:07.993993    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:01:08.009182    6956 logs.go:276] 0 containers: []
W1126 18:01:08.009182    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:01:08.015042    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:01:08.028729    6956 logs.go:276] 2 containers: [1508f888dc1a b49ef773eab6]
I1126 18:01:08.035627    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:01:08.049255    6956 logs.go:276] 0 containers: []
W1126 18:01:08.049255    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:01:08.054967    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:01:08.068460    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:01:08.074339    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:01:08.086136    6956 logs.go:276] 0 containers: []
W1126 18:01:08.086136    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:01:08.086136    6956 logs.go:123] Gathering logs for kube-apiserver [cff135361a59] ...
I1126 18:01:08.086136    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cff135361a59"
I1126 18:01:08.103189    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:08.103189    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:08.126264    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:08.126264    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:08.145619    6956 logs.go:123] Gathering logs for kube-controller-manager [b49ef773eab6] ...
I1126 18:01:08.145619    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b49ef773eab6"
I1126 18:01:08.159455    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:08.159455    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:08.181514    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:08.181514    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:01:08.236006    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:08.614334   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.616483   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.617893   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.619538   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.621042   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:08.614334   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.616483   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.617893   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.619538   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:08.621042   21917 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:08.236006    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:08.236006    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:08.251892    6956 logs.go:123] Gathering logs for etcd [4e6391f0f1e0] ...
I1126 18:01:08.251892    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4e6391f0f1e0"
I1126 18:01:08.268912    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:08.268912    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:08.305081    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:08.305081    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:08.321323    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:08.321323    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:08.364897    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:08.364897    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:08.397733    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:01:08.397733    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:10.960576    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:01:15.974075    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:01:15.981738    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:01:15.995553    6956 logs.go:276] 1 containers: [cff135361a59]
I1126 18:01:16.002075    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:01:16.020855    6956 logs.go:276] 1 containers: [4e6391f0f1e0]
I1126 18:01:16.026657    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:01:16.039778    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:01:16.045579    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:01:16.057809    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:01:16.063534    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:01:16.075334    6956 logs.go:276] 0 containers: []
W1126 18:01:16.075334    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:01:16.081163    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:01:16.093805    6956 logs.go:276] 2 containers: [1508f888dc1a b49ef773eab6]
I1126 18:01:16.099719    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:01:16.110982    6956 logs.go:276] 0 containers: []
W1126 18:01:16.110982    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:01:16.116847    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:01:16.130295    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:01:16.136190    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:01:16.147958    6956 logs.go:276] 0 containers: []
W1126 18:01:16.147958    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:01:16.147958    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:16.147958    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:16.181059    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:01:16.181059    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:16.242752    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:16.242752    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:16.256229    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:16.256229    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:01:16.308025    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:16.687120   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.688714   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.690206   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.691593   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.693108   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:16.687120   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.688714   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.690206   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.691593   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:16.693108   22042 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:16.308025    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:16.308025    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:16.334868    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:16.334868    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:16.355498    6956 logs.go:123] Gathering logs for kube-controller-manager [b49ef773eab6] ...
I1126 18:01:16.355498    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b49ef773eab6"
I1126 18:01:16.370361    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:16.370361    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:16.418628    6956 logs.go:123] Gathering logs for kube-apiserver [cff135361a59] ...
I1126 18:01:16.418628    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cff135361a59"
I1126 18:01:16.435624    6956 logs.go:123] Gathering logs for etcd [4e6391f0f1e0] ...
I1126 18:01:16.435624    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4e6391f0f1e0"
I1126 18:01:16.450411    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:16.450411    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:16.485867    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:16.485867    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:16.503492    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:16.503492    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:19.030773    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:01:24.032530    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:01:24.040061    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:01:24.053560    6956 logs.go:276] 2 containers: [c0725ad6c566 cff135361a59]
I1126 18:01:24.059447    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:01:24.071919    6956 logs.go:276] 1 containers: [4e6391f0f1e0]
I1126 18:01:24.077740    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:01:24.091813    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:01:24.097805    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:01:24.111534    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:01:24.118359    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:01:24.130476    6956 logs.go:276] 0 containers: []
W1126 18:01:24.130495    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:01:24.136914    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:01:24.149358    6956 logs.go:276] 2 containers: [1508f888dc1a b49ef773eab6]
I1126 18:01:24.154766    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:01:24.166787    6956 logs.go:276] 0 containers: []
W1126 18:01:24.167527    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:01:24.173207    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:01:24.184399    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:01:24.190052    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:01:24.201720    6956 logs.go:276] 0 containers: []
W1126 18:01:24.201720    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:01:24.202131    6956 logs.go:123] Gathering logs for etcd [4e6391f0f1e0] ...
I1126 18:01:24.202131    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 4e6391f0f1e0"
I1126 18:01:24.215842    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:24.215842    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:24.242030    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:24.242030    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:24.255976    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:24.255976    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:24.276475    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:24.276475    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:24.289843    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:01:24.289843    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:01:24.305795    6956 logs.go:123] Gathering logs for kube-apiserver [cff135361a59] ...
I1126 18:01:24.305795    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 cff135361a59"
I1126 18:01:24.322667    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:24.322667    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:24.360856    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:24.360856    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:24.373582    6956 logs.go:123] Gathering logs for kube-controller-manager [b49ef773eab6] ...
I1126 18:01:24.373582    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b49ef773eab6"
I1126 18:01:24.386552    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:24.386552    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:24.431002    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:01:24.431002    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:24.496065    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:24.496065    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1126 18:01:38.644355    6956 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (14.1482893s)
W1126 18:01:38.644355    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:34.931187   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": net/http: TLS handshake timeout"
E1127 00:01:39.024166   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:54960->127.0.0.1:8443: read: connection reset by peer"
E1127 00:01:39.025635   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:39.027250   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:39.029119   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:34.931187   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": net/http: TLS handshake timeout"
E1127 00:01:39.024166   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:54960->127.0.0.1:8443: read: connection reset by peer"
E1127 00:01:39.025635   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:39.027250   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:39.029119   22300 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:38.644355    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:38.644355    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:41.194031    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:01:46.205211    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:01:46.212775    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:01:46.224749    6956 logs.go:276] 1 containers: [c0725ad6c566]
I1126 18:01:46.231127    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:01:46.244801    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:01:46.250313    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:01:46.262467    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:01:46.267897    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:01:46.279837    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:01:46.285735    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:01:46.297352    6956 logs.go:276] 0 containers: []
W1126 18:01:46.297352    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:01:46.303173    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:01:46.319231    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:01:46.325166    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:01:46.336365    6956 logs.go:276] 0 containers: []
W1126 18:01:46.336365    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:01:46.343276    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:01:46.356131    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:01:46.361925    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:01:46.374749    6956 logs.go:276] 0 containers: []
W1126 18:01:46.374775    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:01:46.374775    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:46.374775    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:46.386622    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:46.386622    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:01:46.441251    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:46.816616   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.819773   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.821486   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.823475   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.825421   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:46.816616   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.819773   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.821486   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.823475   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:46.825421   22535 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:46.441251    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:01:46.441251    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:01:46.457237    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:46.457237    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:46.493345    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:46.493345    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:46.517948    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:46.517948    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:46.540148    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:01:46.540148    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:46.600102    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:46.600102    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:46.617647    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:01:46.617647    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:01:46.632012    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:46.632012    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:46.646255    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:46.646255    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:46.691915    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:46.691915    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:46.736094    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:01:46.736094    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:01:49.264536    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:01:54.274363    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:01:54.282501    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:01:54.296822    6956 logs.go:276] 1 containers: [c0725ad6c566]
I1126 18:01:54.302687    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:01:54.315810    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:01:54.321772    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:01:54.333119    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:01:54.339362    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:01:54.352358    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:01:54.358171    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:01:54.370584    6956 logs.go:276] 0 containers: []
W1126 18:01:54.370584    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:01:54.376987    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:01:54.389458    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:01:54.395379    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:01:54.405870    6956 logs.go:276] 0 containers: []
W1126 18:01:54.405870    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:01:54.411739    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:01:54.423595    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:01:54.429494    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:01:54.441094    6956 logs.go:276] 0 containers: []
W1126 18:01:54.441094    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:01:54.441094    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:01:54.441094    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:01:54.457943    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:01:54.457943    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:01:54.473011    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:01:54.473011    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:01:54.515858    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:01:54.515858    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:01:54.531276    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:01:54.531276    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:01:54.544547    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:01:54.544547    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:01:54.594988    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:01:54.973743   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.975470   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.976884   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.978463   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.979881   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:01:54.973743   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.975470   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.976884   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.978463   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:01:54.979881   22709 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:01:54.594988    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:01:54.594988    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:01:54.613946    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:01:54.613946    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:01:54.656637    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:01:54.656637    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:01:54.680181    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:01:54.680181    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:01:54.696592    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:01:54.696592    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:01:54.718960    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:01:54.718960    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:01:54.777209    6956 logs.go:123] Gathering logs for container status ...
I1126 18:01:54.777209    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:01:57.319596    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:02:02.330663    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:02:02.338338    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:02:02.355153    6956 logs.go:276] 1 containers: [c0725ad6c566]
I1126 18:02:02.361092    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:02:02.373908    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:02:02.380950    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:02:02.394260    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:02:02.400126    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:02:02.411957    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:02:02.417794    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:02:02.429263    6956 logs.go:276] 0 containers: []
W1126 18:02:02.429263    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:02:02.435170    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:02:02.448668    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:02:02.454950    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:02:02.465662    6956 logs.go:276] 0 containers: []
W1126 18:02:02.465662    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:02:02.471639    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:02:02.483778    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:02:02.490285    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:02:02.502092    6956 logs.go:276] 0 containers: []
W1126 18:02:02.502092    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:02:02.502092    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:02:02.502092    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:02:02.554053    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:02:02.932202   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.934284   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.935887   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.937300   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.938867   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:02:02.932202   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.934284   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.935887   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.937300   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:02.938867   22845 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:02:02.554053    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:02:02.554053    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:02:02.571528    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:02:02.571528    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:02:02.593483    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:02:02.593483    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:02:02.617203    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:02:02.617203    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:02:02.630504    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:02:02.630504    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:02:02.643517    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:02:02.643517    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:02:02.659849    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:02:02.659849    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:02:02.695474    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:02:02.695979    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:02:02.710107    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:02:02.710107    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:02:02.729840    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:02:02.729840    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:02:02.775990    6956 logs.go:123] Gathering logs for container status ...
I1126 18:02:02.776493    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:02:02.807779    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:02:02.807779    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:02:05.374523    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:02:10.375880    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:02:10.383974    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:02:10.397514    6956 logs.go:276] 1 containers: [c0725ad6c566]
I1126 18:02:10.403245    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:02:10.416998    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:02:10.423324    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:02:10.435579    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:02:10.442248    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:02:10.454178    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:02:10.460054    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:02:10.472424    6956 logs.go:276] 0 containers: []
W1126 18:02:10.472424    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:02:10.478249    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:02:10.490870    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:02:10.498259    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:02:10.513225    6956 logs.go:276] 0 containers: []
W1126 18:02:10.513225    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:02:10.519064    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:02:10.536851    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:02:10.544735    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:02:10.556621    6956 logs.go:276] 0 containers: []
W1126 18:02:10.556621    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:02:10.556621    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:02:10.556621    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:02:10.573982    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:02:10.573982    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:02:10.611261    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:02:10.611261    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:02:10.638548    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:02:10.638595    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:02:10.670895    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:02:10.670895    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:02:10.687544    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:02:10.688060    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:02:10.704851    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:02:10.704851    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:02:10.731031    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:02:10.731031    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:02:10.772930    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:02:10.772930    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:02:10.831268    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:02:10.831268    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:02:10.845673    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:02:10.845673    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:02:10.897520    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:02:11.276618   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.278120   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.279618   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.281009   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.282418   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:02:11.276618   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.278120   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.279618   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.281009   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:11.282418   23055 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:02:10.897520    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:02:10.897520    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:02:10.911878    6956 logs.go:123] Gathering logs for container status ...
I1126 18:02:10.911878    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:02:13.467256    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:02:18.469388    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:02:18.477133    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:02:18.490056    6956 logs.go:276] 1 containers: [c0725ad6c566]
I1126 18:02:18.495933    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:02:18.507387    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:02:18.514843    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:02:18.529564    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:02:18.535989    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:02:18.547521    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:02:18.553852    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:02:18.566486    6956 logs.go:276] 0 containers: []
W1126 18:02:18.566486    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:02:18.572322    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:02:18.586353    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:02:18.592242    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:02:18.605100    6956 logs.go:276] 0 containers: []
W1126 18:02:18.605140    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:02:18.612224    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:02:18.625785    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:02:18.631585    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:02:18.643373    6956 logs.go:276] 0 containers: []
W1126 18:02:18.643373    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:02:18.643373    6956 logs.go:123] Gathering logs for container status ...
I1126 18:02:18.643373    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:02:18.677747    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:02:18.677747    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:02:18.691394    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:02:18.691394    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:02:18.706360    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:02:18.706360    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:02:18.744543    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:02:18.744543    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:02:18.775648    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:02:18.775648    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:02:18.790017    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:02:18.790017    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:02:18.809871    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:02:18.809871    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:02:18.853484    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:02:18.853484    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:02:18.913844    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:02:18.913844    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:02:18.965131    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:02:19.343668   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.345379   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.347092   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.348536   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.350004   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:02:19.343668   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.345379   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.347092   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.348536   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:19.350004   23196 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:02:18.965131    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:02:18.965131    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
I1126 18:02:18.987568    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:02:18.987568    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:02:19.007912    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:02:19.007912    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:02:21.531862    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:02:26.547165    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:02:26.554425    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:02:26.570698    6956 logs.go:276] 2 containers: [3cd7e74e2767 c0725ad6c566]
I1126 18:02:26.576585    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:02:26.587632    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:02:26.595874    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:02:26.608468    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:02:26.614271    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:02:26.629417    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:02:26.635269    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:02:26.646836    6956 logs.go:276] 0 containers: []
W1126 18:02:26.646836    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:02:26.652692    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:02:26.667245    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:02:26.673029    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:02:26.686488    6956 logs.go:276] 0 containers: []
W1126 18:02:26.686488    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:02:26.692293    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:02:26.703490    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:02:26.709311    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:02:26.722656    6956 logs.go:276] 0 containers: []
W1126 18:02:26.722689    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:02:26.722689    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:02:26.722689    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:02:26.742824    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:02:26.742824    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:02:26.756343    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:02:26.756343    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:02:26.799729    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:02:26.799729    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:02:26.858559    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:02:26.858559    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:02:26.873345    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:02:26.873345    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:02:26.904278    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:02:26.904278    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1126 18:02:44.721150    6956 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (17.816872s)
W1126 18:02:44.721150    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:02:37.337940   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": net/http: TLS handshake timeout"
E1127 00:02:45.101285   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:40152->127.0.0.1:8443: read: connection reset by peer"
E1127 00:02:45.103012   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:45.104629   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:45.106109   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:02:37.337940   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": net/http: TLS handshake timeout"
E1127 00:02:45.101285   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused - error from a previous attempt: read tcp 127.0.0.1:40152->127.0.0.1:8443: read: connection reset by peer"
E1127 00:02:45.103012   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:45.104629   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:45.106109   23369 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:02:44.721150    6956 logs.go:123] Gathering logs for kube-apiserver [c0725ad6c566] ...
I1126 18:02:44.721150    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 c0725ad6c566"
W1126 18:02:44.732770    6956 logs.go:130] failed kube-apiserver [c0725ad6c566]: command: /bin/bash -c "docker logs --tail 400 c0725ad6c566" /bin/bash -c "docker logs --tail 400 c0725ad6c566": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: c0725ad6c566
 output: 
** stderr ** 
Error response from daemon: No such container: c0725ad6c566

** /stderr **
I1126 18:02:44.732770    6956 logs.go:123] Gathering logs for container status ...
I1126 18:02:44.732770    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:02:44.767619    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:02:44.767619    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:02:44.783155    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:02:44.783155    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:02:44.824504    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:02:44.824521    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:02:44.845602    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:02:44.845602    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:02:44.864899    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:02:44.864899    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:02:47.396057    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:02:52.405188    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:02:52.412769    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:02:52.432881    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:02:52.438418    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:02:52.450103    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:02:52.455979    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:02:52.466859    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:02:52.472709    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:02:52.484795    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:02:52.490625    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:02:52.501986    6956 logs.go:276] 0 containers: []
W1126 18:02:52.501986    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:02:52.507778    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:02:52.519853    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:02:52.525682    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:02:52.537411    6956 logs.go:276] 0 containers: []
W1126 18:02:52.537411    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:02:52.543362    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:02:52.554923    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:02:52.560716    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:02:52.572748    6956 logs.go:276] 0 containers: []
W1126 18:02:52.572748    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:02:52.572748    6956 logs.go:123] Gathering logs for container status ...
I1126 18:02:52.572748    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:02:52.605456    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:02:52.605456    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:02:52.618118    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:02:52.618118    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:02:52.637094    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:02:52.637094    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:02:52.654465    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:02:52.654465    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:02:52.676696    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:02:52.676696    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:02:52.697972    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:02:52.697972    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:02:52.741913    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:02:52.741913    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:02:52.804039    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:02:52.804039    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:02:52.859615    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:02:53.238777   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.240268   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.241759   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.243138   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.244551   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:02:53.238777   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.240268   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.241759   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.243138   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:02:53.244551   23567 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:02:52.859615    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:02:52.859615    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:02:52.895592    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:02:52.895592    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:02:52.923582    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:02:52.923582    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:02:52.944103    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:02:52.944611    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:02:55.472200    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:00.483646    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:00.490354    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:00.507099    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:00.513547    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:00.527525    6956 logs.go:276] 1 containers: [ff0a7db52e1b]
I1126 18:03:00.533848    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:00.546407    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:00.551822    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:00.564592    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:00.570437    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:00.584176    6956 logs.go:276] 0 containers: []
W1126 18:03:00.584176    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:00.589593    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:00.600763    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:03:00.606073    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:00.619469    6956 logs.go:276] 0 containers: []
W1126 18:03:00.619469    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:00.625513    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:00.638969    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:00.644782    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:00.657382    6956 logs.go:276] 0 containers: []
W1126 18:03:00.657382    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:00.657382    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:00.657382    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:00.679907    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:03:00.679907    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:03:00.695744    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:00.695744    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:00.729564    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:00.729564    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:00.791853    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:00.791853    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:00.807810    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:00.807810    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:00.862313    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:01.240226   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.241968   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.243580   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.245563   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.247283   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:01.240226   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.241968   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.243580   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.245563   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:01.247283   23711 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:00.862313    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:00.862313    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:00.885493    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:00.885493    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:00.899907    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:00.899907    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:00.918887    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:00.918887    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:00.964463    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:00.964463    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:00.980721    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:03:00.980721    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
I1126 18:03:00.995657    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:00.995657    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:03.547585    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:08.557528    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:08.563454    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:08.575364    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:08.581213    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:08.599643    6956 logs.go:276] 2 containers: [ad5b1223458c ff0a7db52e1b]
I1126 18:03:08.605590    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:08.619150    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:08.625551    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:08.638514    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:08.647525    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:08.658260    6956 logs.go:276] 0 containers: []
W1126 18:03:08.658260    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:08.664183    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:08.683837    6956 logs.go:276] 2 containers: [27537f0fd9c1 1508f888dc1a]
I1126 18:03:08.689762    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:08.702354    6956 logs.go:276] 0 containers: []
W1126 18:03:08.702354    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:08.708175    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:08.720293    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:08.726072    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:08.738850    6956 logs.go:276] 0 containers: []
W1126 18:03:08.738850    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:08.738850    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:08.738850    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:08.793504    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:09.171998   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.173594   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.175645   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.177165   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.178612   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:09.171998   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.173594   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.175645   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.177165   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:09.178612   23896 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:08.793504    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:08.793504    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:08.837016    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:08.837016    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:08.896415    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:08.896415    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:08.920350    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:08.920350    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:08.942719    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:08.942719    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:08.983232    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:08.983232    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:08.995887    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:08.995887    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:09.014852    6956 logs.go:123] Gathering logs for etcd [ff0a7db52e1b] ...
I1126 18:03:09.014852    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b"
W1126 18:03:09.026471    6956 logs.go:130] failed etcd [ff0a7db52e1b]: command: /bin/bash -c "docker logs --tail 400 ff0a7db52e1b" /bin/bash -c "docker logs --tail 400 ff0a7db52e1b": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: ff0a7db52e1b
 output: 
** stderr ** 
Error response from daemon: No such container: ff0a7db52e1b

** /stderr **
I1126 18:03:09.026471    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:09.026471    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:09.040916    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:09.040916    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:09.055942    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:09.055942    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:09.079224    6956 logs.go:123] Gathering logs for kube-controller-manager [1508f888dc1a] ...
I1126 18:03:09.079224    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1508f888dc1a"
I1126 18:03:09.095281    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:09.095281    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:11.631449    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:16.631976    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:16.638745    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:16.650946    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:16.656590    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:16.668821    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:16.674520    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:16.693797    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:16.699421    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:16.711078    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:16.716683    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:16.728504    6956 logs.go:276] 0 containers: []
W1126 18:03:16.728504    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:16.734130    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:16.746734    6956 logs.go:276] 1 containers: [27537f0fd9c1]
I1126 18:03:16.752836    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:16.765165    6956 logs.go:276] 0 containers: []
W1126 18:03:16.765165    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:16.772054    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:16.784442    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:16.790444    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:16.803887    6956 logs.go:276] 0 containers: []
W1126 18:03:16.803887    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:16.803887    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:16.803887    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:16.817817    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:16.817817    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:16.840206    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:16.840206    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:16.899396    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:16.899396    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:16.929220    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:16.929235    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:16.954791    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:16.954791    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:16.969530    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:16.969530    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:17.007221    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:17.007221    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:17.057931    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:17.057931    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:17.097068    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:17.097068    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:17.109239    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:17.109239    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:17.163481    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:17.538963   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.540979   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.542706   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.545369   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.547802   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:17.538963   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.540979   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.542706   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.545369   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:17.547802   24130 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:17.163481    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:17.163481    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:19.682990    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:24.687782    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:24.695401    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:24.708871    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:24.714745    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:24.727675    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:24.733820    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:24.750695    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:24.757585    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:24.778293    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:24.783645    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:24.794480    6956 logs.go:276] 0 containers: []
W1126 18:03:24.794480    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:24.801462    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:24.813925    6956 logs.go:276] 1 containers: [27537f0fd9c1]
I1126 18:03:24.820565    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:24.831965    6956 logs.go:276] 0 containers: []
W1126 18:03:24.831965    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:24.838943    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:24.852578    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:24.858505    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:24.869422    6956 logs.go:276] 0 containers: []
W1126 18:03:24.869422    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:24.869422    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:24.869422    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:24.886412    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:24.886412    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:24.911569    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:24.911569    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:24.933923    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:24.933923    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:24.970295    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:24.970295    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:25.028673    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:25.028673    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:25.045628    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:25.045628    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:25.084383    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:25.084383    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:25.114037    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:25.114037    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:25.134502    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:25.134502    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:25.181093    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:25.181093    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:25.195876    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:25.195876    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:25.247711    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:25.627305   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.628754   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.630176   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.631575   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.632953   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:25.627305   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.628754   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.630176   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.631575   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:25.632953   24282 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:27.748204    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:32.761562    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:32.768492    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:32.781405    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:32.787212    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:32.800894    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:32.806820    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:32.818640    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:32.824498    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:32.837674    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:32.843346    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:32.856516    6956 logs.go:276] 0 containers: []
W1126 18:03:32.856516    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:32.863480    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:32.879272    6956 logs.go:276] 1 containers: [27537f0fd9c1]
I1126 18:03:32.885206    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:32.897181    6956 logs.go:276] 0 containers: []
W1126 18:03:32.897181    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:32.903117    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:32.914149    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:32.919509    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:32.931368    6956 logs.go:276] 0 containers: []
W1126 18:03:32.931368    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:32.931368    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:32.931368    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:32.999476    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:32.999476    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:33.014898    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:33.014898    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:33.051498    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:33.051498    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:33.079436    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:33.079436    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:33.103280    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:33.103280    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:33.148009    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:33.148009    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:33.180537    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:33.180537    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:33.232630    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:33.611401   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.613016   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.614623   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.616037   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.617626   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:33.611401   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.613016   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.614623   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.616037   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:33.617626   24392 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:33.232630    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:33.232630    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:33.248053    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:33.248053    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:33.263332    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:33.263332    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:33.280162    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:33.280162    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:35.815721    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:40.818470    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:40.825957    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:40.840076    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:40.846050    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:40.858370    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:40.865339    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:40.877921    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:40.884484    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:40.897593    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:40.905562    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:40.918899    6956 logs.go:276] 0 containers: []
W1126 18:03:40.918899    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:40.924679    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:40.940713    6956 logs.go:276] 2 containers: [8266895e4b9d 27537f0fd9c1]
I1126 18:03:40.946300    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:40.958238    6956 logs.go:276] 0 containers: []
W1126 18:03:40.958238    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:40.963930    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:40.977658    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:40.984009    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:40.997212    6956 logs.go:276] 0 containers: []
W1126 18:03:40.997212    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:40.997212    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:40.997212    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:41.010967    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:41.010967    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:41.027025    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:41.027025    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:41.046661    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:41.046688    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:41.106584    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:41.106584    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:41.125583    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:41.125583    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:41.163935    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:41.163935    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:41.190522    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:41.190522    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:41.216698    6956 logs.go:123] Gathering logs for kube-controller-manager [8266895e4b9d] ...
I1126 18:03:41.216698    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8266895e4b9d"
I1126 18:03:41.237782    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:41.237782    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:41.253161    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:41.253161    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:41.298842    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:41.298842    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:41.351583    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:41.729994   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.731644   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.733091   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.734529   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.736087   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:41.729994   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.731644   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.733091   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.734529   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:41.736087   24611 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:41.351583    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:41.351583    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:43.901199    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:48.906881    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:48.914125    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:48.927838    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:48.935353    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:48.948119    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:48.953431    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:48.965641    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:48.971470    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:48.985450    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:48.991277    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:49.007422    6956 logs.go:276] 0 containers: []
W1126 18:03:49.007422    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:49.013180    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:49.026940    6956 logs.go:276] 2 containers: [8266895e4b9d 27537f0fd9c1]
I1126 18:03:49.033367    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:49.045311    6956 logs.go:276] 0 containers: []
W1126 18:03:49.045311    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:49.051129    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:49.064004    6956 logs.go:276] 0 containers: []
W1126 18:03:49.064004    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:49.070373    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:49.082458    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:49.082458    6956 logs.go:123] Gathering logs for kube-controller-manager [8266895e4b9d] ...
I1126 18:03:49.082458    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8266895e4b9d"
I1126 18:03:49.096460    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:49.096484    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:49.109975    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:49.109975    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:49.122013    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:49.122013    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:49.171630    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:49.550765   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.552297   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.553702   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.555153   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.556637   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:49.550765   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.552297   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.553702   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.555153   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:49.556637   24712 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:49.171630    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:49.171630    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:49.187613    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:49.187613    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:49.223655    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:49.223655    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:03:49.252124    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:49.252124    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:49.277162    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:49.277162    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:49.321676    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:49.321690    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:49.382710    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:49.382710    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:49.404152    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:49.404152    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:49.424300    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:49.424300    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:51.976529    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:03:56.989295    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:03:56.996478    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1126 18:03:57.010931    6956 logs.go:276] 1 containers: [3cd7e74e2767]
I1126 18:03:57.016755    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1126 18:03:57.028778    6956 logs.go:276] 1 containers: [ad5b1223458c]
I1126 18:03:57.034644    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1126 18:03:57.046555    6956 logs.go:276] 1 containers: [dedb6d07fe7a]
I1126 18:03:57.052531    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1126 18:03:57.064490    6956 logs.go:276] 2 containers: [5d2760b8e041 ab104b9920bf]
I1126 18:03:57.071427    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1126 18:03:57.087572    6956 logs.go:276] 0 containers: []
W1126 18:03:57.087572    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:03:57.093468    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1126 18:03:57.106502    6956 logs.go:276] 2 containers: [8266895e4b9d 27537f0fd9c1]
I1126 18:03:57.113453    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1126 18:03:57.127788    6956 logs.go:276] 0 containers: []
W1126 18:03:57.127788    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:03:57.134724    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I1126 18:03:57.147378    6956 logs.go:276] 1 containers: [6c51274772f7]
I1126 18:03:57.152708    6956 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1126 18:03:57.165319    6956 logs.go:276] 0 containers: []
W1126 18:03:57.165319    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:03:57.165319    6956 logs.go:123] Gathering logs for kube-controller-manager [8266895e4b9d] ...
I1126 18:03:57.165319    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8266895e4b9d"
I1126 18:03:57.178519    6956 logs.go:123] Gathering logs for controller_ingress [6c51274772f7] ...
I1126 18:03:57.178519    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6c51274772f7"
I1126 18:03:57.200006    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:03:57.200006    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:03:57.264524    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:03:57.264524    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:03:57.318152    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:03:57.695182   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.696956   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.698799   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.700981   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.702438   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:03:57.695182   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.696956   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.698799   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.700981   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:03:57.702438   24859 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:03:57.318152    6956 logs.go:123] Gathering logs for kube-apiserver [3cd7e74e2767] ...
I1126 18:03:57.318152    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 3cd7e74e2767"
I1126 18:03:57.336979    6956 logs.go:123] Gathering logs for kube-scheduler [ab104b9920bf] ...
I1126 18:03:57.336979    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ab104b9920bf"
I1126 18:03:57.359357    6956 logs.go:123] Gathering logs for kube-controller-manager [27537f0fd9c1] ...
I1126 18:03:57.359357    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 27537f0fd9c1"
I1126 18:03:57.373463    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:03:57.373463    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:03:57.415596    6956 logs.go:123] Gathering logs for container status ...
I1126 18:03:57.415596    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1126 18:03:57.450789    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:03:57.450789    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:03:57.462527    6956 logs.go:123] Gathering logs for etcd [ad5b1223458c] ...
I1126 18:03:57.462527    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ad5b1223458c"
I1126 18:03:57.478677    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a] ...
I1126 18:03:57.478677    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 dedb6d07fe7a"
I1126 18:03:57.515171    6956 logs.go:123] Gathering logs for kube-scheduler [5d2760b8e041] ...
I1126 18:03:57.515171    6956 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5d2760b8e041"
I1126 18:04:00.048009    6956 api_server.go:253] Checking apiserver healthz at https://172.30.130.31:8443/healthz ...
I1126 18:04:05.062038    6956 api_server.go:269] stopped: https://172.30.130.31:8443/healthz: Get "https://172.30.130.31:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1126 18:04:05.064019    6956 kubeadm.go:597] duration metric: took 4m8.4396969s to restartPrimaryControlPlane
W1126 18:04:05.064019    6956 out.go:270] ü§¶  Unable to restart control-plane node(s), will reset cluster: <no value>
I1126 18:04:05.064539    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I1126 18:04:16.096385    6956 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (11.0318454s)
I1126 18:04:16.111832    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1126 18:04:16.137728    6956 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1126 18:04:16.160719    6956 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1126 18:04:16.169067    6956 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1126 18:04:16.169067    6956 kubeadm.go:157] found existing configuration files:

I1126 18:04:16.183441    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1126 18:04:16.190733    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1126 18:04:16.204723    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1126 18:04:16.230166    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1126 18:04:16.239067    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1126 18:04:16.254559    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1126 18:04:16.276046    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1126 18:04:16.283393    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1126 18:04:16.298249    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1126 18:04:16.322414    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1126 18:04:16.330510    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1126 18:04:16.346354    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1126 18:04:16.354718    6956 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1126 18:04:16.386640    6956 kubeadm.go:310] W1127 00:04:16.774357   25106 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1126 18:04:16.386640    6956 kubeadm.go:310] W1127 00:04:16.774945   25106 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1126 18:04:16.456805    6956 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1126 18:08:18.165254    6956 kubeadm.go:310] error execution phase wait-control-plane: could not initialize a Kubernetes cluster
I1126 18:08:18.166305    6956 kubeadm.go:310] To see the stack trace of this error execute with --v=5 or higher
I1126 18:08:18.166305    6956 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1126 18:08:18.166305    6956 kubeadm.go:310] [preflight] Running pre-flight checks
I1126 18:08:18.166305    6956 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1126 18:08:18.166824    6956 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1126 18:08:18.166824    6956 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1126 18:08:18.166824    6956 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1126 18:08:18.174142    6956 out.go:235]     ‚ñ™ Generando certificados y llaves
I1126 18:08:18.176231    6956 kubeadm.go:310] [certs] Using existing ca certificate authority
I1126 18:08:18.176231    6956 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1126 18:08:18.176231    6956 kubeadm.go:310] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing front-proxy-ca certificate authority
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing front-proxy-client certificate and key on disk
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing etcd/ca certificate authority
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing etcd/server certificate and key on disk
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing etcd/peer certificate and key on disk
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I1126 18:08:18.176749    6956 kubeadm.go:310] [certs] Using existing apiserver-etcd-client certificate and key on disk
I1126 18:08:18.177259    6956 kubeadm.go:310] [certs] Using the existing "sa" key
I1126 18:08:18.177296    6956 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1126 18:08:18.177296    6956 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1126 18:08:18.177296    6956 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1126 18:08:18.177296    6956 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1126 18:08:18.177837    6956 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1126 18:08:18.177837    6956 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1126 18:08:18.177837    6956 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1126 18:08:18.177837    6956 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1126 18:08:18.182597    6956 out.go:235]     ‚ñ™ Iniciando plano de control
I1126 18:08:18.183125    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1126 18:08:18.183125    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1126 18:08:18.183663    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1126 18:08:18.183663    6956 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1126 18:08:18.183663    6956 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1126 18:08:18.183663    6956 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1126 18:08:18.184200    6956 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1126 18:08:18.184200    6956 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1126 18:08:18.184200    6956 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.431611ms
I1126 18:08:18.184200    6956 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1126 18:08:18.184200    6956 kubeadm.go:310] [api-check] The API server is not healthy after 4m0.000648265s
I1126 18:08:18.184200    6956 kubeadm.go:310] 
I1126 18:08:18.184707    6956 kubeadm.go:310] Unfortunately, an error has occurred:
I1126 18:08:18.184720    6956 kubeadm.go:310] 	context deadline exceeded
I1126 18:08:18.184720    6956 kubeadm.go:310] 
I1126 18:08:18.184720    6956 kubeadm.go:310] This error is likely caused by:
I1126 18:08:18.184720    6956 kubeadm.go:310] 	- The kubelet is not running
I1126 18:08:18.184720    6956 kubeadm.go:310] 	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
I1126 18:08:18.184720    6956 kubeadm.go:310] 
I1126 18:08:18.184720    6956 kubeadm.go:310] If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
I1126 18:08:18.184720    6956 kubeadm.go:310] 	- 'systemctl status kubelet'
I1126 18:08:18.184720    6956 kubeadm.go:310] 	- 'journalctl -xeu kubelet'
I1126 18:08:18.184720    6956 kubeadm.go:310] 
I1126 18:08:18.185248    6956 kubeadm.go:310] Additionally, a control plane component may have crashed or exited when started by the container runtime.
I1126 18:08:18.185248    6956 kubeadm.go:310] To troubleshoot, list all containers using your preferred container runtimes CLI.
I1126 18:08:18.185248    6956 kubeadm.go:310] Here is one example how you may list all running Kubernetes containers by using crictl:
I1126 18:08:18.185248    6956 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I1126 18:08:18.185248    6956 kubeadm.go:310] 	Once you have found the failing container, you can inspect its logs with:
I1126 18:08:18.185771    6956 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
W1126 18:08:18.189406    6956 out.go:270] üí¢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.431611ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is not healthy after 4m0.000648265s

Unfortunately, an error has occurred:
	context deadline exceeded

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W1127 00:04:16.774357   25106 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W1127 00:04:16.774945   25106 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: could not initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

I1126 18:08:18.195524    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I1126 18:08:29.165242    6956 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (10.9697181s)
I1126 18:08:29.185837    6956 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1126 18:08:29.212440    6956 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1126 18:08:29.222042    6956 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1126 18:08:29.222042    6956 kubeadm.go:157] found existing configuration files:

I1126 18:08:29.237536    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1126 18:08:29.245377    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1126 18:08:29.260330    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1126 18:08:29.285150    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1126 18:08:29.294073    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1126 18:08:29.310207    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1126 18:08:29.334175    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1126 18:08:29.342134    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1126 18:08:29.356493    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1126 18:08:29.383828    6956 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1126 18:08:29.391939    6956 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1126 18:08:29.408452    6956 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1126 18:08:29.416677    6956 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I1126 18:08:29.447349    6956 kubeadm.go:310] W1127 00:08:29.835097   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1126 18:08:29.447454    6956 kubeadm.go:310] W1127 00:08:29.836039   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1126 18:08:29.515261    6956 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1126 18:12:30.947965    6956 kubeadm.go:310] error execution phase wait-control-plane: could not initialize a Kubernetes cluster
I1126 18:12:30.947965    6956 kubeadm.go:310] To see the stack trace of this error execute with --v=5 or higher
I1126 18:12:30.949717    6956 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1126 18:12:30.949717    6956 kubeadm.go:310] [preflight] Running pre-flight checks
I1126 18:12:30.949717    6956 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1126 18:12:30.949717    6956 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1126 18:12:30.950257    6956 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1126 18:12:30.950257    6956 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1126 18:12:30.958426    6956 out.go:235]     ‚ñ™ Generando certificados y llaves
I1126 18:12:30.958426    6956 kubeadm.go:310] [certs] Using existing ca certificate authority
I1126 18:12:30.958970    6956 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1126 18:12:30.958970    6956 kubeadm.go:310] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I1126 18:12:30.958970    6956 kubeadm.go:310] [certs] Using existing front-proxy-ca certificate authority
I1126 18:12:30.958970    6956 kubeadm.go:310] [certs] Using existing front-proxy-client certificate and key on disk
I1126 18:12:30.958970    6956 kubeadm.go:310] [certs] Using existing etcd/ca certificate authority
I1126 18:12:30.959502    6956 kubeadm.go:310] [certs] Using existing etcd/server certificate and key on disk
I1126 18:12:30.959502    6956 kubeadm.go:310] [certs] Using existing etcd/peer certificate and key on disk
I1126 18:12:30.959502    6956 kubeadm.go:310] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I1126 18:12:30.959502    6956 kubeadm.go:310] [certs] Using existing apiserver-etcd-client certificate and key on disk
I1126 18:12:30.959502    6956 kubeadm.go:310] [certs] Using the existing "sa" key
I1126 18:12:30.959502    6956 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1126 18:12:30.959502    6956 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1126 18:12:30.960037    6956 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1126 18:12:30.960037    6956 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1126 18:12:30.960037    6956 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1126 18:12:30.960037    6956 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1126 18:12:30.960037    6956 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1126 18:12:30.960037    6956 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1126 18:12:30.967384    6956 out.go:235]     ‚ñ™ Iniciando plano de control
I1126 18:12:30.968101    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1126 18:12:30.968101    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1126 18:12:30.968615    6956 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1126 18:12:30.968677    6956 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1126 18:12:30.968677    6956 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1126 18:12:30.968677    6956 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1126 18:12:30.968677    6956 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1126 18:12:30.969232    6956 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1126 18:12:30.969232    6956 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.534349ms
I1126 18:12:30.969232    6956 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1126 18:12:30.969232    6956 kubeadm.go:310] [api-check] The API server is not healthy after 4m0.000797646s
I1126 18:12:30.969232    6956 kubeadm.go:310] 
I1126 18:12:30.969232    6956 kubeadm.go:310] Unfortunately, an error has occurred:
I1126 18:12:30.969232    6956 kubeadm.go:310] 	context deadline exceeded
I1126 18:12:30.969232    6956 kubeadm.go:310] 
I1126 18:12:30.969232    6956 kubeadm.go:310] This error is likely caused by:
I1126 18:12:30.969232    6956 kubeadm.go:310] 	- The kubelet is not running
I1126 18:12:30.969743    6956 kubeadm.go:310] 	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
I1126 18:12:30.969743    6956 kubeadm.go:310] 
I1126 18:12:30.969788    6956 kubeadm.go:310] If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
I1126 18:12:30.969788    6956 kubeadm.go:310] 	- 'systemctl status kubelet'
I1126 18:12:30.969788    6956 kubeadm.go:310] 	- 'journalctl -xeu kubelet'
I1126 18:12:30.969788    6956 kubeadm.go:310] 
I1126 18:12:30.969788    6956 kubeadm.go:310] Additionally, a control plane component may have crashed or exited when started by the container runtime.
I1126 18:12:30.969788    6956 kubeadm.go:310] To troubleshoot, list all containers using your preferred container runtimes CLI.
I1126 18:12:30.969788    6956 kubeadm.go:310] Here is one example how you may list all running Kubernetes containers by using crictl:
I1126 18:12:30.970339    6956 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
I1126 18:12:30.970339    6956 kubeadm.go:310] 	Once you have found the failing container, you can inspect its logs with:
I1126 18:12:30.970339    6956 kubeadm.go:310] 	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
I1126 18:12:30.971418    6956 kubeadm.go:394] duration metric: took 12m34.3888256s to StartCluster
I1126 18:12:30.979076    6956 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I1126 18:12:31.003024    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I1126 18:12:31.036326    6956 cri.go:89] found id: "4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1"
I1126 18:12:31.036856    6956 cri.go:89] found id: ""
I1126 18:12:31.037380    6956 logs.go:276] 1 containers: [4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1]
I1126 18:12:31.043726    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.046300    6956 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I1126 18:12:31.060970    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I1126 18:12:31.090729    6956 cri.go:89] found id: "843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695"
I1126 18:12:31.090729    6956 cri.go:89] found id: ""
I1126 18:12:31.090729    6956 logs.go:276] 1 containers: [843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695]
I1126 18:12:31.095038    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.097947    6956 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I1126 18:12:31.114459    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I1126 18:12:31.142700    6956 cri.go:89] found id: "dedb6d07fe7a0a62587935aad60c3d5f9870adfe5c7c7a4626efc37b10177669"
I1126 18:12:31.142700    6956 cri.go:89] found id: ""
I1126 18:12:31.142700    6956 logs.go:276] 1 containers: [dedb6d07fe7a0a62587935aad60c3d5f9870adfe5c7c7a4626efc37b10177669]
I1126 18:12:31.147406    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.150269    6956 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I1126 18:12:31.164191    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I1126 18:12:31.196580    6956 cri.go:89] found id: "52f72ae48d1a49fdc77194a3b7f1e461a82a1568657939862d547f19c7ead480"
I1126 18:12:31.196580    6956 cri.go:89] found id: ""
I1126 18:12:31.196580    6956 logs.go:276] 1 containers: [52f72ae48d1a49fdc77194a3b7f1e461a82a1568657939862d547f19c7ead480]
I1126 18:12:31.200968    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.203868    6956 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I1126 18:12:31.218367    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I1126 18:12:31.246039    6956 cri.go:89] found id: ""
I1126 18:12:31.246039    6956 logs.go:276] 0 containers: []
W1126 18:12:31.246039    6956 logs.go:278] No container was found matching "kube-proxy"
I1126 18:12:31.246834    6956 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I1126 18:12:31.261094    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I1126 18:12:31.289151    6956 cri.go:89] found id: "24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f"
I1126 18:12:31.289151    6956 cri.go:89] found id: "1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d"
I1126 18:12:31.289151    6956 cri.go:89] found id: ""
I1126 18:12:31.289151    6956 logs.go:276] 2 containers: [24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f 1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d]
I1126 18:12:31.293481    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.301888    6956 ssh_runner.go:195] Run: which crictl
I1126 18:12:31.304637    6956 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I1126 18:12:31.318544    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I1126 18:12:31.349030    6956 cri.go:89] found id: ""
I1126 18:12:31.349030    6956 logs.go:276] 0 containers: []
W1126 18:12:31.349030    6956 logs.go:278] No container was found matching "kindnet"
I1126 18:12:31.349030    6956 cri.go:54] listing CRI containers in root : {State:all Name:controller_ingress Namespaces:[]}
I1126 18:12:31.364032    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=controller_ingress
I1126 18:12:31.394464    6956 cri.go:89] found id: ""
I1126 18:12:31.394497    6956 logs.go:276] 0 containers: []
W1126 18:12:31.394497    6956 logs.go:278] No container was found matching "controller_ingress"
I1126 18:12:31.394497    6956 cri.go:54] listing CRI containers in root : {State:all Name:storage-provisioner Namespaces:[]}
I1126 18:12:31.408375    6956 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=storage-provisioner
I1126 18:12:31.436673    6956 cri.go:89] found id: ""
I1126 18:12:31.436673    6956 logs.go:276] 0 containers: []
W1126 18:12:31.436673    6956 logs.go:278] No container was found matching "storage-provisioner"
I1126 18:12:31.437217    6956 logs.go:123] Gathering logs for coredns [dedb6d07fe7a0a62587935aad60c3d5f9870adfe5c7c7a4626efc37b10177669] ...
I1126 18:12:31.437217    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 dedb6d07fe7a0a62587935aad60c3d5f9870adfe5c7c7a4626efc37b10177669"
I1126 18:12:31.491345    6956 logs.go:123] Gathering logs for kube-controller-manager [1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d] ...
I1126 18:12:31.491345    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d"
I1126 18:12:31.517905    6956 logs.go:123] Gathering logs for Docker ...
I1126 18:12:31.517905    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1126 18:12:31.565090    6956 logs.go:123] Gathering logs for kubelet ...
I1126 18:12:31.565090    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1126 18:12:31.626786    6956 logs.go:123] Gathering logs for dmesg ...
I1126 18:12:31.626786    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1126 18:12:31.646288    6956 logs.go:123] Gathering logs for describe nodes ...
I1126 18:12:31.646288    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W1126 18:12:31.699290    6956 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:12:32.078385   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.079908   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.081297   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.082959   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.084491   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E1127 00:12:32.078385   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.079908   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.081297   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.082959   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:12:32.084491   27820 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I1126 18:12:31.699290    6956 logs.go:123] Gathering logs for etcd [843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695] ...
I1126 18:12:31.699290    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695"
I1126 18:12:31.728074    6956 logs.go:123] Gathering logs for kube-apiserver [4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1] ...
I1126 18:12:31.728074    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1"
I1126 18:12:31.756545    6956 logs.go:123] Gathering logs for kube-scheduler [52f72ae48d1a49fdc77194a3b7f1e461a82a1568657939862d547f19c7ead480] ...
I1126 18:12:31.756545    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 52f72ae48d1a49fdc77194a3b7f1e461a82a1568657939862d547f19c7ead480"
I1126 18:12:31.798929    6956 logs.go:123] Gathering logs for kube-controller-manager [24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f] ...
I1126 18:12:31.798929    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f"
I1126 18:12:31.828913    6956 logs.go:123] Gathering logs for container status ...
I1126 18:12:31.829461    6956 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
W1126 18:12:31.866799    6956 out.go:418] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.534349ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is not healthy after 4m0.000797646s

Unfortunately, an error has occurred:
	context deadline exceeded

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W1127 00:08:29.835097   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W1127 00:08:29.836039   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: could not initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
W1126 18:12:31.867909    6956 out.go:270] 
W1126 18:12:31.868953    6956 out.go:270] üí£  No se ha podido iniciar el cl√∫ster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.534349ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is not healthy after 4m0.000797646s

Unfortunately, an error has occurred:
	context deadline exceeded

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W1127 00:08:29.835097   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W1127 00:08:29.836039   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: could not initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W1126 18:12:31.872672    6956 out.go:270] 
W1126 18:12:31.881736    6956 out.go:293] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I1126 18:12:31.894060    6956 out.go:201] 
W1126 18:12:31.898059    6956 out.go:270] ‚ùå  Saliendo por un error K8S_KUBELET_NOT_RUNNING: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.534349ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is not healthy after 4m0.000797646s

Unfortunately, an error has occurred:
	context deadline exceeded

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W1127 00:08:29.835097   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W1127 00:08:29.836039   26513 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: could not initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W1126 18:12:31.902084    6956 out.go:270] üí°  Suggestion: Comprueba la salida de 'journalctl -xeu kubelet', intenta pasar --extra-config=kubelet.cgroup-driver=systemd a minikube start
W1126 18:12:31.902328    6956 out.go:270] üçø  Related issue: https://github.com/kubernetes/minikube/issues/4172
I1126 18:12:31.907281    6956 out.go:201] 


==> Docker <==
Nov 27 00:09:56 minikube dockerd[20463]: time="2024-11-27T00:09:56.295309965Z" level=info msg="shim disconnected" id=4c0c1699775d23db3fde29db1ab752f2c602755f88d8ddc14ddfd61fb83ba404 namespace=moby
Nov 27 00:09:56 minikube dockerd[20463]: time="2024-11-27T00:09:56.295364165Z" level=warning msg="cleaning up after shim disconnected" id=4c0c1699775d23db3fde29db1ab752f2c602755f88d8ddc14ddfd61fb83ba404 namespace=moby
Nov 27 00:09:56 minikube dockerd[20463]: time="2024-11-27T00:09:56.295371565Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:09:57 minikube dockerd[20463]: time="2024-11-27T00:09:57.495632771Z" level=info msg="shim disconnected" id=a3050485f0d20f08c481a4892adf4ca05a439df51bca629ea66add7dea0c2eb4 namespace=moby
Nov 27 00:09:57 minikube dockerd[20457]: time="2024-11-27T00:09:57.495780471Z" level=info msg="ignoring event" container=a3050485f0d20f08c481a4892adf4ca05a439df51bca629ea66add7dea0c2eb4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:09:57 minikube dockerd[20463]: time="2024-11-27T00:09:57.496601372Z" level=warning msg="cleaning up after shim disconnected" id=a3050485f0d20f08c481a4892adf4ca05a439df51bca629ea66add7dea0c2eb4 namespace=moby
Nov 27 00:09:57 minikube dockerd[20463]: time="2024-11-27T00:09:57.496684873Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:10:04 minikube dockerd[20457]: time="2024-11-27T00:10:04.049498983Z" level=info msg="ignoring event" container=fcfc5cc3e8420d5b3dba11dffe6cabba66ee0799e1bdd8d9117bebd7ab5e7a1b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.049594083Z" level=info msg="shim disconnected" id=fcfc5cc3e8420d5b3dba11dffe6cabba66ee0799e1bdd8d9117bebd7ab5e7a1b namespace=moby
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.049628483Z" level=warning msg="cleaning up after shim disconnected" id=fcfc5cc3e8420d5b3dba11dffe6cabba66ee0799e1bdd8d9117bebd7ab5e7a1b namespace=moby
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.049634584Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.592428125Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.592510725Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.592532225Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:10:04 minikube dockerd[20463]: time="2024-11-27T00:10:04.592782025Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:10:23 minikube dockerd[20463]: time="2024-11-27T00:10:23.217858875Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:10:23 minikube dockerd[20463]: time="2024-11-27T00:10:23.217938275Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:10:23 minikube dockerd[20463]: time="2024-11-27T00:10:23.217949275Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:10:23 minikube dockerd[20463]: time="2024-11-27T00:10:23.218017675Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:10:43 minikube dockerd[20457]: time="2024-11-27T00:10:43.598348393Z" level=info msg="ignoring event" container=7b0eb47a22c8b893faaef95395fdcfb3e8825cac330901fe9dd1b4cad2bb5c42 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:10:43 minikube dockerd[20463]: time="2024-11-27T00:10:43.598549893Z" level=info msg="shim disconnected" id=7b0eb47a22c8b893faaef95395fdcfb3e8825cac330901fe9dd1b4cad2bb5c42 namespace=moby
Nov 27 00:10:43 minikube dockerd[20463]: time="2024-11-27T00:10:43.598588094Z" level=warning msg="cleaning up after shim disconnected" id=7b0eb47a22c8b893faaef95395fdcfb3e8825cac330901fe9dd1b4cad2bb5c42 namespace=moby
Nov 27 00:10:43 minikube dockerd[20463]: time="2024-11-27T00:10:43.598594794Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:10:43 minikube dockerd[20463]: time="2024-11-27T00:10:43.607301713Z" level=warning msg="cleanup warnings time=\"2024-11-27T00:10:43Z\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n" namespace=moby
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.208158909Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.208210409Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.208218709Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.208270609Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:11:25 minikube dockerd[20457]: time="2024-11-27T00:11:25.268995063Z" level=info msg="ignoring event" container=843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.269739365Z" level=info msg="shim disconnected" id=843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695 namespace=moby
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.270084966Z" level=warning msg="cleaning up after shim disconnected" id=843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695 namespace=moby
Nov 27 00:11:25 minikube dockerd[20463]: time="2024-11-27T00:11:25.270147466Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:11:27 minikube dockerd[20463]: time="2024-11-27T00:11:27.218476319Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:11:27 minikube dockerd[20463]: time="2024-11-27T00:11:27.218543319Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:11:27 minikube dockerd[20463]: time="2024-11-27T00:11:27.218554719Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:11:27 minikube dockerd[20463]: time="2024-11-27T00:11:27.218618319Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:11:36 minikube dockerd[20457]: time="2024-11-27T00:11:36.484351046Z" level=info msg="ignoring event" container=1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:11:36 minikube dockerd[20463]: time="2024-11-27T00:11:36.485359848Z" level=info msg="shim disconnected" id=1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d namespace=moby
Nov 27 00:11:36 minikube dockerd[20463]: time="2024-11-27T00:11:36.485413749Z" level=warning msg="cleaning up after shim disconnected" id=1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d namespace=moby
Nov 27 00:11:36 minikube dockerd[20463]: time="2024-11-27T00:11:36.485420349Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:11:47 minikube dockerd[20457]: time="2024-11-27T00:11:47.487552098Z" level=info msg="ignoring event" container=4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:11:47 minikube dockerd[20463]: time="2024-11-27T00:11:47.488088900Z" level=info msg="shim disconnected" id=4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1 namespace=moby
Nov 27 00:11:47 minikube dockerd[20463]: time="2024-11-27T00:11:47.488280100Z" level=warning msg="cleaning up after shim disconnected" id=4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1 namespace=moby
Nov 27 00:11:47 minikube dockerd[20463]: time="2024-11-27T00:11:47.488331301Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:11:59 minikube dockerd[20463]: time="2024-11-27T00:11:59.212021556Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:11:59 minikube dockerd[20463]: time="2024-11-27T00:11:59.212101656Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:11:59 minikube dockerd[20463]: time="2024-11-27T00:11:59.212115456Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:11:59 minikube dockerd[20463]: time="2024-11-27T00:11:59.212189256Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:13:19 minikube dockerd[20463]: time="2024-11-27T00:13:19.221549107Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Nov 27 00:13:19 minikube dockerd[20463]: time="2024-11-27T00:13:19.221602007Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Nov 27 00:13:19 minikube dockerd[20463]: time="2024-11-27T00:13:19.221614007Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:13:19 minikube dockerd[20463]: time="2024-11-27T00:13:19.221689907Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Nov 27 00:13:30 minikube dockerd[20457]: time="2024-11-27T00:13:30.773892528Z" level=info msg="ignoring event" container=24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:13:30 minikube dockerd[20463]: time="2024-11-27T00:13:30.774816831Z" level=info msg="shim disconnected" id=24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f namespace=moby
Nov 27 00:13:30 minikube dockerd[20463]: time="2024-11-27T00:13:30.774896731Z" level=warning msg="cleaning up after shim disconnected" id=24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f namespace=moby
Nov 27 00:13:30 minikube dockerd[20463]: time="2024-11-27T00:13:30.774904431Z" level=info msg="cleaning up dead shim" namespace=moby
Nov 27 00:13:39 minikube dockerd[20457]: time="2024-11-27T00:13:39.605487776Z" level=info msg="ignoring event" container=2672b102c41a27880538844e51b9d5586373890cc78d011da89b195c9fbcd552 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 27 00:13:39 minikube dockerd[20463]: time="2024-11-27T00:13:39.605861077Z" level=info msg="shim disconnected" id=2672b102c41a27880538844e51b9d5586373890cc78d011da89b195c9fbcd552 namespace=moby
Nov 27 00:13:39 minikube dockerd[20463]: time="2024-11-27T00:13:39.606037677Z" level=warning msg="cleaning up after shim disconnected" id=2672b102c41a27880538844e51b9d5586373890cc78d011da89b195c9fbcd552 namespace=moby
Nov 27 00:13:39 minikube dockerd[20463]: time="2024-11-27T00:13:39.606129778Z" level=info msg="cleaning up dead shim" namespace=moby


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
2672b102c41a2       604f5db92eaa8                                                                                                                23 seconds ago       Exited              kube-apiserver            5                   7feef2901d459       kube-apiserver-minikube
24a3badfe697f       045733566833c                                                                                                                About a minute ago   Exited              kube-controller-manager   2                   798f379d44c24       kube-controller-manager-minikube
843fa0b11e054       2e96e5913fc06                                                                                                                2 minutes ago        Exited              etcd                      5                   304f9553c9540       etcd-minikube
52f72ae48d1a4       1766f54c897f0                                                                                                                5 minutes ago        Running             kube-scheduler            0                   eee92dc1dbc15       kube-scheduler-minikube
20954705c5901       e0a4585f718e4                                                                                                                4 days ago           Exited              hydra                     0                   8d81209184dd0       hydra-5bcb598d8d-w49x2
78670b748532c       e0a4585f718e4                                                                                                                4 days ago           Exited              hydra-automigrate         0                   7990809151f3d       hydra-automigrate-txjdd
5c262135cbfe4       oryd/hydra-maester@sha256:19c9c68049e4860a646a979342a1f27842f6898c2179aaf9f73fd87305897c75                                   4 days ago           Exited              hydra-maester             0                   9395b80fee350       hydra-hydra-maester-755b9d54c5-krkh9
6bfa8bba37996       kicbase/echo-server@sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6                                  4 days ago           Exited              hello-world-app           0                   672e441970f40       hello-world-app-55bf9c44b4-4b5wd
6c51274772f70       registry.k8s.io/ingress-nginx/controller@sha256:d5f8217feeac4887cb1ed21f27c2674e58be06bd8f5184cacea2a69abaf78dce             4 days ago           Exited              controller                0                   15d25ac5e56d7       ingress-nginx-controller-bc57996ff-4p9c7
3d98f133d3677       ce263a8653f9c                                                                                                                4 days ago           Exited              patch                     1                   5953f645741ed       ingress-nginx-admission-patch-nsx6c
fc2165d4f1c22       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a320a50cc91bd15fd2d6fa6de58bd98c1bd64b9a6f926ce23a600d87043455a3   4 days ago           Exited              create                    0                   97b0bbfd5302b       ingress-nginx-admission-create-9jbhf
816e78382c0e6       bitnami/postgresql@sha256:bfd5e759a97e06833bb8b740184d9d899eb7401081341abd04d0cce1035b98c7                                   4 days ago           Exited              postgresql                0                   ad632ab2caf16       hydra-postgresql-0
8712c95190699       localhost:5000/groups-api@sha256:2aa7894b4dd1bdfa2a50ee89662e272ed073cf933939d73c4726dc068a9f19a8                            4 days ago           Exited              groups-api                0                   75fc37d33e7ff       groups-api-5fc7c586f-v4hnp
896e83884ac40       d436f3da039df                                                                                                                4 days ago           Exited              mongodb                   0                   3941d6831ad4f       my-mongodb-78477b8c65-mkczz
f2b8e87dfbb89       bitnami/mongodb@sha256:5b1b1409240696cf687f894c553f1ff6b98d4508f23e76aafc7e3d05871cb114                                      4 days ago           Exited              log-dir                   0                   3941d6831ad4f       my-mongodb-78477b8c65-mkczz
26561cc056598       localhost:5000/users-api@sha256:2a08e8a1c0bfa974eda065733fe48bf5ee7dd57e9546b7b48f8ea223ba788772                             4 days ago           Exited              users-api                 0                   cbe50150d90cc       users-api-546fdbc755-l97vk
5a17971807827       registry@sha256:ac0192b549007e22998eb74e8d8488dcfe70f1489520c3b144a6047ac5efbe90                                             4 days ago           Exited              registry                  0                   b779c17402e8f       registry-66c9cd494c-xhztf
32cb016b1105d       gcr.io/k8s-minikube/kube-registry-proxy@sha256:b3fa0b2df8737fdb85ad5918a7e2652527463e357afff83a5e5bb966bcedc367              4 days ago           Exited              registry-proxy            0                   04cae36552c24       registry-proxy-5kcnt
6f7819aa728a2       mysql@sha256:3e3ed60f669764b8331d0dc1f9718a25cd9af59c587b8d28c4853becc20d9735                                                4 days ago           Exited              mysql                     0                   5fcc483769068       mysql-8445d8574c-4ql5h
dedb6d07fe7a0       cbb01a7bd410d                                                                                                                4 days ago           Exited              coredns                   0                   eaf6085add98c       coredns-6f6b679f8f-bcwxm


==> controller_ingress [6c51274772f7] <==
I1122 06:25:21.232410       7 main.go:107] "successfully validated configuration, accepting" ingress="authentication/hydra-public"
I1122 06:25:21.253399       7 store.go:440] "Found valid IngressClass" ingress="authentication/hydra-admin" ingressclass="nginx"
I1122 06:25:21.272779       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-admin", UID:"bb81499d-aff5-4e2c-988a-9365835e1984", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7079", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1122 06:25:21.275669       7 store.go:440] "Found valid IngressClass" ingress="authentication/hydra-public" ingressclass="nginx"
I1122 06:25:21.276519       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-public", UID:"c18765df-5a5b-4c80-9569-e38df1e44cdd", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7080", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1122 06:25:23.989274       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:25:23.990230       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
I1122 06:25:23.993485       7 controller.go:193] "Configuration changes detected, backend reload required"
I1122 06:25:24.110547       7 controller.go:213] "Backend successfully reloaded"
I1122 06:25:24.117673       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-bc57996ff-4p9c7", UID:"c00bffa5-0941-42c5-bd6e-86b9d0015c5c", APIVersion:"v1", ResourceVersion:"5977", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W1122 06:25:32.978199       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:25:32.978331       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1122 06:25:36.316195       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:25:36.316285       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
I1122 06:26:20.112730       7 status.go:304] "updating Ingress status" namespace="authentication" ingress="hydra-admin" currentValue=null newValue=[{"ip":"172.30.130.31"}]
I1122 06:26:20.112730       7 status.go:304] "updating Ingress status" namespace="authentication" ingress="hydra-public" currentValue=null newValue=[{"ip":"172.30.130.31"}]
I1122 06:26:20.172466       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-public", UID:"c18765df-5a5b-4c80-9569-e38df1e44cdd", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7176", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1122 06:26:20.176665       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-admin", UID:"bb81499d-aff5-4e2c-988a-9365835e1984", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7177", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1122 06:26:20.194833       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:26:20.194928       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1122 06:36:34.009698       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:36:34.010477       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
I1122 06:36:34.147013       7 admission.go:149] processed ingress via admission controller {testedIngressLength:3 testedIngressTime:0.146s renderingIngressLength:3 renderingIngressTime:0.006s admissionTime:0.155s testedConfigurationSize:30.2kB}
I1122 06:36:34.149159       7 main.go:107] "successfully validated configuration, accepting" ingress="authentication/hydra-admin"
W1122 06:36:34.185016       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:36:34.185066       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1122 06:36:34.192348       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1122 06:36:34.192446       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
I1122 06:36:34.196692       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-admin", UID:"bb81499d-aff5-4e2c-988a-9365835e1984", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7811", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I1122 06:36:34.231411       7 admission.go:149] processed ingress via admission controller {testedIngressLength:3 testedIngressTime:0.039s renderingIngressLength:3 renderingIngressTime:0.001s admissionTime:0.04s testedConfigurationSize:30.2kB}
I1122 06:36:34.231472       7 main.go:107] "successfully validated configuration, accepting" ingress="authentication/hydra-public"
I1122 06:36:34.240530       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"authentication", Name:"hydra-public", UID:"c18765df-5a5b-4c80-9569-e38df1e44cdd", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7813", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W1122 06:36:37.518547       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:36:37.518662       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1122 06:36:40.865931       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1122 06:36:40.866567       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
E1123 03:20:40.951897       7 leaderelection.go:340] Failed to update lock optimitically: Put "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded, falling back to slow path
E1123 03:20:40.975060       7 leaderelection.go:347] error retrieving resource lock ingress-nginx/ingress-nginx-leader: client rate limiter Wait returned an error: context deadline exceeded
I1123 03:20:40.982040       7 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E1123 03:20:41.002206       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I1123 03:20:41.014277       7 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I1123 03:20:55.175351       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
W1123 03:24:40.224102       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1123 03:24:40.228689       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1123 03:24:43.530498       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1123 03:24:43.530793       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
W1123 03:24:46.865013       7 controller.go:1216] Service "authentication/hydra-public" does not have any active Endpoint.
W1123 03:24:46.865478       7 controller.go:1216] Service "authentication/hydra-admin" does not have any active Endpoint.
172.30.128.1 - - [22/Nov/2024:06:48:01 +0000] "POST /admin/clients HTTP/1.1" 201 1318 "-" "PostmanRuntime/7.42.0" 345 0.220 [authentication-hydra-admin-http] [] 10.244.0.17:4445 1318 0.220 201 e83ba69277e0c2dafad3d7c93df7bd20
172.30.128.1 - - [22/Nov/2024:06:53:37 +0000] "POST /admin/clients HTTP/1.1" 201 1307 "-" "PostmanRuntime/7.42.0" 537 0.151 [authentication-hydra-admin-http] [] 10.244.0.17:4445 1307 0.151 201 a92d37519debdd0c0435a191df0251df
172.30.128.1 - fer-read-client [22/Nov/2024:06:58:27 +0000] "POST /oauth2/token HTTP/1.1" 200 1161 "-" "PostmanRuntime/7.42.0" 430 1.932 [authentication-hydra-public-http] [] 10.244.0.17:4444 1161 1.932 200 189470e4c91918f192cae4952d805b47
172.30.128.1 - - [22/Nov/2024:07:00:10 +0000] "POST /admin/clients HTTP/1.1" 409 121 "-" "PostmanRuntime/7.42.0" 537 0.404 [authentication-hydra-admin-http] [] 10.244.0.17:4445 121 0.404 409 2dfa9d8dd1db15d8574309b788c750fd
10.244.0.1 - - [22/Nov/2024:07:07:57 +0000] "GET /.well-known/openid-configuration HTTP/1.1" 200 1868 "-" "Microsoft ASP.NET Core JwtBearer handler" 247 6.871 [authentication-hydra-public-http] [] 10.244.0.17:4444 1868 6.871 200 63019cff5a4a5aaa27bf5209a4b29b34
10.244.0.1 - - [22/Nov/2024:07:07:57 +0000] "GET /.well-known/jwks.json HTTP/1.1" 200 1583 "-" "Microsoft ASP.NET Core JwtBearer handler" 236 0.057 [authentication-hydra-public-http] [] 10.244.0.17:4444 1583 0.057 200 4e50c037fb95872075160278a75b8d42
172.30.128.1 - fer-read-client [22/Nov/2024:07:12:17 +0000] "POST /oauth2/token HTTP/1.1" 400 275 "-" "PostmanRuntime/7.42.0" 428 0.145 [authentication-hydra-public-http] [] 10.244.0.17:4444 275 0.145 400 cfa7425f932310c8dc7d2ba95b2253a1
172.30.128.1 - fer-read-client [22/Nov/2024:07:13:41 +0000] "POST /oauth2/token HTTP/1.1" 400 275 "-" "PostmanRuntime/7.42.0" 428 0.183 [authentication-hydra-public-http] [] 10.244.0.17:4444 275 0.183 400 c34597ed1e911cad2be96f85131b2e8d
172.30.128.1 - - [22/Nov/2024:07:15:03 +0000] "POST /admin/clients HTTP/1.1" 201 1309 "-" "PostmanRuntime/7.42.0" 538 0.178 [authentication-hydra-admin-http] [] 10.244.0.17:4445 1309 0.178 201 5e0bd078225133cbdb5dee7a8d714910
172.30.128.1 - fer-write-client [22/Nov/2024:07:15:26 +0000] "POST /oauth2/token HTTP/1.1" 200 1163 "-" "PostmanRuntime/7.42.0" 433 0.261 [authentication-hydra-public-http] [] 10.244.0.17:4444 1163 0.261 200 e0fe26a15b62e72f0f3d57c5a04ea7ba
172.30.128.1 - - [22/Nov/2024:07:18:38 +0000] "POST /admin/clients HTTP/1.1" 409 121 "-" "PostmanRuntime/7.42.0" 537 0.123 [authentication-hydra-admin-http] [] 10.244.0.17:4445 121 0.123 409 27b03afe8c0b597edc4e0fc7a67e3134
172.30.128.1 - fer-read-client [22/Nov/2024:07:19:04 +0000] "POST /oauth2/token HTTP/1.1" 200 1161 "-" "PostmanRuntime/7.42.0" 430 0.364 [authentication-hydra-public-http] [] 10.244.0.17:4444 1161 0.364 200 31180724c28e9a2a3f046d228e6a0216


==> coredns [dedb6d07fe7a] <==
[INFO] 10.244.0.12:42610 - 8461 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000052374s
[INFO] 10.244.0.12:42610 - 29576 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000135735s
[INFO] 10.244.0.12:40722 - 58240 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.001910081s
[INFO] 10.244.0.12:42610 - 31272 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000922356s
[INFO] 10.244.0.12:48271 - 36119 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.014131738s
[INFO] 10.244.0.12:40934 - 51954 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.00038591s
[INFO] 10.244.0.12:40934 - 6850 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.018367652s
[INFO] 10.244.0.12:40934 - 59831 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000641184s
[INFO] 10.244.0.12:40934 - 27331 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000187208s
[INFO] 10.244.0.12:48271 - 53861 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.001291364s
[INFO] 10.244.0.12:48271 - 46782 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000096752s
[INFO] 10.244.0.12:48271 - 26191 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000106847s
[INFO] 10.244.0.12:40934 - 24789 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.001593415s
[INFO] 10.244.0.12:48271 - 18744 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000208097s
[INFO] 10.244.0.12:48271 - 23140 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.00022259s
[INFO] 10.244.0.12:40934 - 34592 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.001976026s
[INFO] 10.244.0.12:40934 - 24929 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.003209518s
[INFO] 10.244.0.12:48271 - 40167 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.00294405s
[INFO] 10.244.0.12:42701 - 44528 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.011040214s
[INFO] 10.244.0.12:49033 - 45110 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.010979944s
[INFO] 10.244.0.12:49033 - 63589 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000590806s
[INFO] 10.244.0.12:42701 - 58667 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000526939s
[INFO] 10.244.0.12:42701 - 48913 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000163319s
[INFO] 10.244.0.12:49033 - 27766 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000315244s
[INFO] 10.244.0.12:42701 - 1499 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000141929s
[INFO] 10.244.0.12:49033 - 9940 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000096752s
[INFO] 10.244.0.12:42701 - 3183 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000095552s
[INFO] 10.244.0.12:49033 - 34769 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000076762s
[INFO] 10.244.0.12:42701 - 38564 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000179011s
[INFO] 10.244.0.12:49033 - 43802 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000086957s
[INFO] 10.244.0.12:49033 - 38299 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000653675s
[INFO] 10.244.0.12:42701 - 12692 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.001579315s
[INFO] 10.244.0.12:54405 - 43630 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.025803938s
[INFO] 10.244.0.12:40240 - 32993 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.0259691s
[INFO] 10.244.0.12:40240 - 49818 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000305015s
[INFO] 10.244.0.12:54405 - 5161 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000064024s
[INFO] 10.244.0.12:54405 - 20771 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000169964s
[INFO] 10.244.0.12:40240 - 3404 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000151858s
[INFO] 10.244.0.12:54405 - 46037 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000260799s
[INFO] 10.244.0.12:54405 - 42893 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000146556s
[INFO] 10.244.0.12:40240 - 58558 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000041316s
[INFO] 10.244.0.12:54405 - 10019 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000078829s
[INFO] 10.244.0.12:40240 - 63718 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000068425s
[INFO] 10.244.0.12:40240 - 13037 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000229087s
[INFO] 10.244.0.12:54405 - 39364 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.005388934s
[INFO] 10.244.0.12:40240 - 12939 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.004374251s
[INFO] 10.244.0.12:34967 - 2813 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000233218s
[INFO] 10.244.0.12:54434 - 17480 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.03294692s
[INFO] 10.244.0.12:34967 - 54985 "A IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.035949449s
[INFO] 10.244.0.12:54434 - 56696 "AAAA IN hello-world-app.default.svc.cluster.local.ingress-nginx.svc.cluster.local. udp 91 false 512" NXDOMAIN qr,aa,rd 184 0.000158812s
[INFO] 10.244.0.12:34967 - 29204 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000437033s
[INFO] 10.244.0.12:34967 - 9612 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000149112s
[INFO] 10.244.0.12:34967 - 56760 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000168313s
[INFO] 10.244.0.12:34967 - 33388 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000139511s
[INFO] 10.244.0.12:54434 - 59434 "A IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001424609s
[INFO] 10.244.0.12:54434 - 5887 "AAAA IN hello-world-app.default.svc.cluster.local.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.001153389s
[INFO] 10.244.0.12:54434 - 36139 "A IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000089107s
[INFO] 10.244.0.12:54434 - 34961 "AAAA IN hello-world-app.default.svc.cluster.local.cluster.local. udp 73 false 512" NXDOMAIN qr,aa,rd 166 0.000058804s
[INFO] 10.244.0.12:34967 - 9516 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.004685559s
[INFO] 10.244.0.12:54434 - 549 "A IN hello-world-app.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.002303276s


==> describe nodes <==
command /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" failed with error: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E1127 00:13:42.448610   28063 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:13:42.450074   28063 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:13:42.451636   28063 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:13:42.453975   28063 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
E1127 00:13:42.455576   28063 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://localhost:8443/api?timeout=32s\": dial tcp 127.0.0.1:8443: connect: connection refused"
The connection to the server localhost:8443 was refused - did you specify the right host or port?


==> dmesg <==
[ +22.222064] kauditd_printk_skb: 65 callbacks suppressed
[ +21.399893] kauditd_printk_skb: 2 callbacks suppressed
[Nov26 23:37] kauditd_printk_skb: 2 callbacks suppressed
[Nov26 23:38] kauditd_printk_skb: 2 callbacks suppressed
[Nov26 23:41] systemd-fstab-generator[9597]: Ignoring "noauto" option for root device
[  +0.359064] systemd-fstab-generator[9632]: Ignoring "noauto" option for root device
[  +0.142531] systemd-fstab-generator[9644]: Ignoring "noauto" option for root device
[  +0.151262] systemd-fstab-generator[9658]: Ignoring "noauto" option for root device
[  +2.948279] systemd-fstab-generator[10065]: Ignoring "noauto" option for root device
[  +0.120119] systemd-fstab-generator[10077]: Ignoring "noauto" option for root device
[  +0.122814] systemd-fstab-generator[10089]: Ignoring "noauto" option for root device
[  +0.174000] systemd-fstab-generator[10104]: Ignoring "noauto" option for root device
[  +0.512994] systemd-fstab-generator[10228]: Ignoring "noauto" option for root device
[  +0.717640] kauditd_printk_skb: 245 callbacks suppressed
[  +1.785064] systemd-fstab-generator[10742]: Ignoring "noauto" option for root device
[Nov26 23:42] kauditd_printk_skb: 62 callbacks suppressed
[Nov26 23:46] systemd-fstab-generator[14648]: Ignoring "noauto" option for root device
[  +1.239142] kauditd_printk_skb: 24 callbacks suppressed
[ +19.405991] kauditd_printk_skb: 37 callbacks suppressed
[ +22.878133] kauditd_printk_skb: 2 callbacks suppressed
[Nov26 23:49] kauditd_printk_skb: 1 callbacks suppressed
[Nov26 23:50] systemd-fstab-generator[16045]: Ignoring "noauto" option for root device
[ +22.059734] kauditd_printk_skb: 65 callbacks suppressed
[Nov26 23:51] kauditd_printk_skb: 2 callbacks suppressed
[Nov26 23:52] kauditd_printk_skb: 1 callbacks suppressed
[Nov26 23:56] systemd-fstab-generator[17907]: Ignoring "noauto" option for root device
[  +0.379099] systemd-fstab-generator[17943]: Ignoring "noauto" option for root device
[  +0.140045] systemd-fstab-generator[17955]: Ignoring "noauto" option for root device
[  +0.143859] systemd-fstab-generator[17970]: Ignoring "noauto" option for root device
[  +2.987814] systemd-fstab-generator[18373]: Ignoring "noauto" option for root device
[Nov26 23:57] systemd-fstab-generator[18385]: Ignoring "noauto" option for root device
[  +0.119198] systemd-fstab-generator[18397]: Ignoring "noauto" option for root device
[  +0.167853] systemd-fstab-generator[18412]: Ignoring "noauto" option for root device
[  +0.478031] systemd-fstab-generator[18536]: Ignoring "noauto" option for root device
[  +0.572892] kauditd_printk_skb: 240 callbacks suppressed
[  +1.881760] systemd-fstab-generator[19159]: Ignoring "noauto" option for root device
[ +21.660320] kauditd_printk_skb: 73 callbacks suppressed
[Nov26 23:59] systemd-fstab-generator[20202]: Ignoring "noauto" option for root device
[  +0.351258] systemd-fstab-generator[20239]: Ignoring "noauto" option for root device
[  +0.071494] kauditd_printk_skb: 17 callbacks suppressed
[  +0.070577] systemd-fstab-generator[20251]: Ignoring "noauto" option for root device
[  +0.147642] systemd-fstab-generator[20265]: Ignoring "noauto" option for root device
[ +10.137557] kauditd_printk_skb: 62 callbacks suppressed
[  +2.594235] systemd-fstab-generator[20705]: Ignoring "noauto" option for root device
[  +0.117289] systemd-fstab-generator[20718]: Ignoring "noauto" option for root device
[  +0.130717] systemd-fstab-generator[20729]: Ignoring "noauto" option for root device
[  +0.177919] systemd-fstab-generator[20744]: Ignoring "noauto" option for root device
[  +0.490898] systemd-fstab-generator[20892]: Ignoring "noauto" option for root device
[  +1.903525] systemd-fstab-generator[21000]: Ignoring "noauto" option for root device
[  +0.058664] kauditd_printk_skb: 137 callbacks suppressed
[Nov27 00:00] kauditd_printk_skb: 53 callbacks suppressed
[Nov27 00:02] kauditd_printk_skb: 2 callbacks suppressed
[Nov27 00:04] systemd-fstab-generator[25133]: Ignoring "noauto" option for root device
[ +22.128124] kauditd_printk_skb: 65 callbacks suppressed
[ +21.646827] kauditd_printk_skb: 2 callbacks suppressed
[Nov27 00:07] kauditd_printk_skb: 1 callbacks suppressed
[Nov27 00:08] systemd-fstab-generator[26541]: Ignoring "noauto" option for root device
[ +21.479984] kauditd_printk_skb: 65 callbacks suppressed
[Nov27 00:09] kauditd_printk_skb: 2 callbacks suppressed
[Nov27 00:11] kauditd_printk_skb: 1 callbacks suppressed


==> etcd [843fa0b11e05] <==
{"level":"warn","ts":"2024-11-27T00:11:25.258771Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-27T00:11:25.258885Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.30.130.31:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.30.130.31:2380","--initial-cluster=minikube=https://172.30.130.31:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.30.130.31:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.30.130.31:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-11-27T00:11:25.258943Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-27T00:11:25.258952Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://172.30.130.31:2380"]}
{"level":"info","ts":"2024-11-27T00:11:25.258972Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"error","ts":"2024-11-27T00:11:25.259092Z","caller":"embed/etcd.go:538","msg":"creating peer listener failed","error":"listen tcp 172.30.130.31:2380: bind: cannot assign requested address","stacktrace":"go.etcd.io/etcd/server/v3/embed.configurePeerListeners\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:538\ngo.etcd.io/etcd/server/v3/embed.StartEtcd\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:132\ngo.etcd.io/etcd/server/v3/etcdmain.startEtcd\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:228\ngo.etcd.io/etcd/server/v3/etcdmain.startEtcdOrProxyV2\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:135\ngo.etcd.io/etcd/server/v3/etcdmain.Main\n\tgo.etcd.io/etcd/server/v3/etcdmain/main.go:40\nmain.main\n\tgo.etcd.io/etcd/server/v3/main.go:31\nruntime.main\n\truntime/proc.go:267"}
{"level":"info","ts":"2024-11-27T00:11:25.259146Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://172.30.130.31:2380"],"advertise-client-urls":["https://172.30.130.31:2379"]}
{"level":"info","ts":"2024-11-27T00:11:25.259157Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://172.30.130.31:2380"],"advertise-client-urls":["https://172.30.130.31:2379"]}
{"level":"warn","ts":"2024-11-27T00:11:25.259169Z","caller":"etcdmain/etcd.go:146","msg":"failed to start etcd","error":"listen tcp 172.30.130.31:2380: bind: cannot assign requested address"}
{"level":"fatal","ts":"2024-11-27T00:11:25.259180Z","caller":"etcdmain/etcd.go:204","msg":"discovery failed","error":"listen tcp 172.30.130.31:2380: bind: cannot assign requested address","stacktrace":"go.etcd.io/etcd/server/v3/etcdmain.startEtcdOrProxyV2\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:204\ngo.etcd.io/etcd/server/v3/etcdmain.Main\n\tgo.etcd.io/etcd/server/v3/etcdmain/main.go:40\nmain.main\n\tgo.etcd.io/etcd/server/v3/main.go:31\nruntime.main\n\truntime/proc.go:267"}


==> kernel <==
 00:13:42 up 15:14,  0 users,  load average: 0.04, 0.14, 0.20
Linux minikube 5.10.207 #1 SMP Tue Sep 3 21:45:30 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [2672b102c41a] <==
I1127 00:13:19.294227       1 options.go:228] external host was not specified, using 172.30.130.31
I1127 00:13:19.295657       1 server.go:142] Version: v1.31.0
I1127 00:13:19.295695       1 server.go:144] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
W1127 00:13:19.582076       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:19.582660       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I1127 00:13:19.582795       1 shared_informer.go:313] Waiting for caches to sync for node_authorizer
I1127 00:13:19.589122       1 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1127 00:13:19.591483       1 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1127 00:13:19.591506       1 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1127 00:13:19.591667       1 instance.go:232] Using reconciler: lease
W1127 00:13:19.592663       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:20.582895       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:20.583026       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:20.594018       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:21.881732       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:22.330937       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:22.467609       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:24.527457       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:24.720732       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:24.853751       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:28.000761       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:28.504131       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:29.570777       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:34.830209       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:36.211538       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 00:13:37.036084       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
F1127 00:13:39.592104       1 instance.go:225] Error creating leases: error creating storage factory: context deadline exceeded


==> kube-controller-manager [24a3badfe697] <==
I1127 00:11:59.526147       1 serving.go:386] Generated self-signed cert in-memory
I1127 00:11:59.751694       1 controllermanager.go:197] "Starting" version="v1.31.0"
I1127 00:11:59.751723       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 00:11:59.753992       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1127 00:11:59.754141       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1127 00:11:59.754269       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1127 00:11:59.754308       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1127 00:13:30.759218       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://172.30.130.31:8443/healthz\": dial tcp 172.30.130.31:8443: i/o timeout"


==> kube-scheduler [52f72ae48d1a] <==
W1127 00:12:17.924080       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://172.30.130.31:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:17.924154       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://172.30.130.31:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:19.158662       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://172.30.130.31:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:19.158801       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://172.30.130.31:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:19.628371       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:19.628490       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:19.831425       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://172.30.130.31:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:19.831520       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://172.30.130.31:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:19.963889       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://172.30.130.31:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:19.964042       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.130.31:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:21.140382       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://172.30.130.31:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:21.140477       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://172.30.130.31:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:21.369983       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:21.370128       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:21.499319       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://172.30.130.31:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:21.499479       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://172.30.130.31:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:21.958705       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://172.30.130.31:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:21.958923       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://172.30.130.31:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:22.216332       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.30.130.31:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:22.216524       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.30.130.31:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:22.546182       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://172.30.130.31:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:22.546321       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://172.30.130.31:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:22.928601       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.30.130.31:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:22.928754       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.30.130.31:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:23.301790       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:23.302260       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:23.402463       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://172.30.130.31:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:23.402735       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://172.30.130.31:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:12:25.528282       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:12:25.528380       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:04.132332       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://172.30.130.31:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:04.132426       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://172.30.130.31:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:06.383367       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://172.30.130.31:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:06.383480       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://172.30.130.31:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:06.685158       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:06.685260       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:06.691535       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:06.691605       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:06.744338       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.30.130.31:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:06.744433       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://172.30.130.31:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:08.526283       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://172.30.130.31:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:08.526424       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://172.30.130.31:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:09.493074       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://172.30.130.31:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:09.493210       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://172.30.130.31:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:09.501764       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://172.30.130.31:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:09.501894       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://172.30.130.31:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:10.989811       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://172.30.130.31:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:10.989858       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://172.30.130.31:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:12.814627       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:12.814796       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:13.456114       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://172.30.130.31:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:13.456246       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://172.30.130.31:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:14.331131       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://172.30.130.31:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:14.331253       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://172.30.130.31:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:15.518242       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://172.30.130.31:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:15.518311       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://172.30.130.31:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:15.879536       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://172.30.130.31:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:15.879675       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://172.30.130.31:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
W1127 00:13:16.964618       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://172.30.130.31:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
E1127 00:13:16.964768       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://172.30.130.31:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"


==> kubelet <==
Nov 27 00:13:00 minikube kubelet[26548]: W1127 00:13:00.104057   26548 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
Nov 27 00:13:00 minikube kubelet[26548]: E1127 00:13:00.104258   26548 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:00 minikube kubelet[26548]: E1127 00:13:00.687537   26548 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="7s"
Nov 27 00:13:01 minikube kubelet[26548]: E1127 00:13:01.146583   26548 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Nov 27 00:13:05 minikube kubelet[26548]: E1127 00:13:05.734977   26548 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:06 minikube kubelet[26548]: E1127 00:13:06.074995   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:06 minikube kubelet[26548]: I1127 00:13:06.075571   26548 scope.go:117] "RemoveContainer" containerID="4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1"
Nov 27 00:13:06 minikube kubelet[26548]: E1127 00:13:06.075689   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=kube-apiserver pod=kube-apiserver-minikube_kube-system(3b477dd9d597348c2d756ddaf052d2e0)\"" pod="kube-system/kube-apiserver-minikube" podUID="3b477dd9d597348c2d756ddaf052d2e0"
Nov 27 00:13:09 minikube kubelet[26548]: E1127 00:13:09.075746   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:09 minikube kubelet[26548]: I1127 00:13:09.076453   26548 scope.go:117] "RemoveContainer" containerID="843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695"
Nov 27 00:13:09 minikube kubelet[26548]: E1127 00:13:09.076653   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=etcd pod=etcd-minikube_kube-system(d986ff9085865150b8537500c2893c95)\"" pod="kube-system/etcd-minikube" podUID="d986ff9085865150b8537500c2893c95"
Nov 27 00:13:11 minikube kubelet[26548]: E1127 00:13:11.147128   26548 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Nov 27 00:13:14 minikube kubelet[26548]: W1127 00:13:14.695039   26548 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
Nov 27 00:13:14 minikube kubelet[26548]: E1127 00:13:14.695127   26548 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:14 minikube kubelet[26548]: W1127 00:13:14.835685   26548 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
Nov 27 00:13:14 minikube kubelet[26548]: E1127 00:13:14.835904   26548 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:17 minikube kubelet[26548]: E1127 00:13:17.689411   26548 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="7s"
Nov 27 00:13:19 minikube kubelet[26548]: E1127 00:13:19.075380   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:19 minikube kubelet[26548]: I1127 00:13:19.076381   26548 scope.go:117] "RemoveContainer" containerID="4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1"
Nov 27 00:13:19 minikube kubelet[26548]: E1127 00:13:19.487271   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:20 minikube kubelet[26548]: E1127 00:13:20.495993   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:21 minikube kubelet[26548]: E1127 00:13:21.147966   26548 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Nov 27 00:13:23 minikube kubelet[26548]: E1127 00:13:23.075523   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:23 minikube kubelet[26548]: I1127 00:13:23.076356   26548 scope.go:117] "RemoveContainer" containerID="843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695"
Nov 27 00:13:23 minikube kubelet[26548]: E1127 00:13:23.076498   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=etcd pod=etcd-minikube_kube-system(d986ff9085865150b8537500c2893c95)\"" pod="kube-system/etcd-minikube" podUID="d986ff9085865150b8537500c2893c95"
Nov 27 00:13:23 minikube kubelet[26548]: E1127 00:13:23.302125   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:25 minikube kubelet[26548]: E1127 00:13:25.075423   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:25 minikube kubelet[26548]: W1127 00:13:25.394549   26548 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 172.30.130.31:8443: i/o timeout
Nov 27 00:13:25 minikube kubelet[26548]: E1127 00:13:25.394673   26548 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:27 minikube kubelet[26548]: E1127 00:13:27.075010   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:27 minikube kubelet[26548]: E1127 00:13:27.846728   26548 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 172.30.130.31:8443: i/o timeout" node="minikube"
Nov 27 00:13:31 minikube kubelet[26548]: E1127 00:13:31.083126   26548 iptables.go:577] "Could not set up iptables canary" err=<
Nov 27 00:13:31 minikube kubelet[26548]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Nov 27 00:13:31 minikube kubelet[26548]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Nov 27 00:13:31 minikube kubelet[26548]:         Perhaps ip6tables or your kernel needs to be upgraded.
Nov 27 00:13:31 minikube kubelet[26548]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Nov 27 00:13:31 minikube kubelet[26548]: E1127 00:13:31.148393   26548 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Nov 27 00:13:31 minikube kubelet[26548]: I1127 00:13:31.149375   26548 scope.go:117] "RemoveContainer" containerID="1aac04d8eba0c796ca55e2a9ea2860d6ac3140a8c3fb1b3a46e62e2f9dafc97d"
Nov 27 00:13:31 minikube kubelet[26548]: E1127 00:13:31.607277   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:31 minikube kubelet[26548]: I1127 00:13:31.608007   26548 scope.go:117] "RemoveContainer" containerID="24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f"
Nov 27 00:13:31 minikube kubelet[26548]: E1127 00:13:31.608134   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(6e07439efa4d6dab7ea55fed1a35452a)\"" pod="kube-system/kube-controller-manager-minikube" podUID="6e07439efa4d6dab7ea55fed1a35452a"
Nov 27 00:13:34 minikube kubelet[26548]: E1127 00:13:34.623799   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:34 minikube kubelet[26548]: I1127 00:13:34.625148   26548 scope.go:117] "RemoveContainer" containerID="24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f"
Nov 27 00:13:34 minikube kubelet[26548]: E1127 00:13:34.625284   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(6e07439efa4d6dab7ea55fed1a35452a)\"" pod="kube-system/kube-controller-manager-minikube" podUID="6e07439efa4d6dab7ea55fed1a35452a"
Nov 27 00:13:34 minikube kubelet[26548]: E1127 00:13:34.691133   26548 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="7s"
Nov 27 00:13:34 minikube kubelet[26548]: E1127 00:13:34.848449   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:34 minikube kubelet[26548]: I1127 00:13:34.849407   26548 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Nov 27 00:13:37 minikube kubelet[26548]: E1127 00:13:37.075135   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:37 minikube kubelet[26548]: I1127 00:13:37.076130   26548 scope.go:117] "RemoveContainer" containerID="843fa0b11e05441aa0056a90cf07d78f6fafc65aec75aa66d65a0540ef7f8695"
Nov 27 00:13:37 minikube kubelet[26548]: E1127 00:13:37.076276   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=etcd pod=etcd-minikube_kube-system(d986ff9085865150b8537500c2893c95)\"" pod="kube-system/etcd-minikube" podUID="d986ff9085865150b8537500c2893c95"
Nov 27 00:13:37 minikube kubelet[26548]: E1127 00:13:37.736242   26548 certificate_manager.go:562] "Unhandled Error" err="kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post \"https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests\": dial tcp 172.30.130.31:8443: i/o timeout" logger="UnhandledError"
Nov 27 00:13:38 minikube kubelet[26548]: E1127 00:13:38.578649   26548 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events\": dial tcp 172.30.130.31:8443: i/o timeout" event="&Event{ObjectMeta:{minikube.180baaa5def062ad  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:minikube,UID:minikube,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2024-11-27 00:08:31.040709293 +0000 UTC m=+0.211574504,LastTimestamp:2024-11-27 00:08:31.040709293 +0000 UTC m=+0.211574504,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Nov 27 00:13:39 minikube kubelet[26548]: I1127 00:13:39.702904   26548 scope.go:117] "RemoveContainer" containerID="4d9fadccde3ca7fef9d4ee66c40222d7dbcea763a3483311892dae6b352231d1"
Nov 27 00:13:39 minikube kubelet[26548]: E1127 00:13:39.703070   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:39 minikube kubelet[26548]: I1127 00:13:39.703864   26548 scope.go:117] "RemoveContainer" containerID="2672b102c41a27880538844e51b9d5586373890cc78d011da89b195c9fbcd552"
Nov 27 00:13:39 minikube kubelet[26548]: E1127 00:13:39.703985   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=kube-apiserver pod=kube-apiserver-minikube_kube-system(3b477dd9d597348c2d756ddaf052d2e0)\"" pod="kube-system/kube-apiserver-minikube" podUID="3b477dd9d597348c2d756ddaf052d2e0"
Nov 27 00:13:40 minikube kubelet[26548]: E1127 00:13:40.988994   26548 kubelet_node_status.go:706] "Failed to set some node status fields" err="failed to validate nodeIP: node IP: \"172.30.130.31\" not found in the host's network interfaces" node="minikube"
Nov 27 00:13:40 minikube kubelet[26548]: I1127 00:13:40.990143   26548 scope.go:117] "RemoveContainer" containerID="24a3badfe697f092e4edeb7782b1fedb49969d68e3a471781a6883f81816182f"
Nov 27 00:13:40 minikube kubelet[26548]: E1127 00:13:40.990261   26548 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(6e07439efa4d6dab7ea55fed1a35452a)\"" pod="kube-system/kube-controller-manager-minikube" podUID="6e07439efa4d6dab7ea55fed1a35452a"
Nov 27 00:13:41 minikube kubelet[26548]: E1127 00:13:41.149539   26548 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"

